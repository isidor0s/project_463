<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd"><article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="abstract"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Yale J Biol Med</journal-id><journal-title-group><journal-title>The Yale Journal of Biology and Medicine</journal-title></journal-title-group><issn pub-type="ppub">0044-0086</issn><issn pub-type="epub">1551-4056</issn><publisher><publisher-name>Yale Journal of Biology and Medicine</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmc">2496697</article-id><article-categories><subj-group subj-group-type="heading"><subject>Thesis Abstracts</subject></subj-group></article-categories><title-group><article-title>Yale University School of Medicine</article-title><subtitle>2008 student thesis abstracts</subtitle></title-group><pub-date pub-type="epub"><month>8</month><year>2008</year></pub-date><pub-date pub-type="ppub"><month>6</month><year>2008</year></pub-date><volume>81</volume><issue>2</issue><fpage>59</fpage><lpage>102</lpage><permissions><copyright-statement>Copyright &#x000a9;2008, Yale Journal of Biology and Medicine</copyright-statement><copyright-year>2008</copyright-year><copyright-holder>Yale Journal of Biology and Medicine</copyright-holder><license license-type="open-access"><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial No Derivatives License, which permits for noncommercial use, distribution, and reproduction in any digital medium, provided the original work is properly cited and is not altered in any way.</license-p></license></permissions></article-meta></front><sub-article id="d31e53" article-type="abstract"><front-stub><title-group><article-title>The Uses of Rickets: Race, Technology, and the Politics of Preventive Medicine in the Early 20th Century</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Arwady</surname><given-names>M. Allison</given-names></name></contrib></contrib-group><aff>Department of the History of Medicine, Yale University School of Medicine, New Haven, Connecticut</aff><author-notes><fn><p>Sponsored by John H. Warner</p></fn></author-notes></front-stub><body><p>Rickets, the bone disease classically caused by vitamin D deficiency, was one of the most common diseases of children 100 years ago. It has been recognized as a disease of urban living and linked to issues of race and culture for generations. This paper uses unpublished patient records from 1904 to 1909 and archival and published materials from multiple community-based trials, including the New Haven Rickets Study (1923-1926), to explore how the definition, diagnosis, and treatment of rickets shifted in the first decades of the 20th century in the United States.</p><p>Before 1910, as evidenced by patient records, neither the diagnosis nor the treatment of rickets had been standardized. The disease frequently was presented as a disease of African Americans or Italian immigrants and used to reinforce racial stereotypes, promote the assimilation of immigrants into majority cultures, and call for behavioral change. In the second and third decades of the 20th century, as clinicians and scientists unraveled the twin roles of diet and sunlight exposure in the disease&#x02019;s etiology, both diagnosis and treatment became more standardized. But this standardization &#x02014; including exchanging bedside diagnosis for X-ray technology and promoting general preventive measures &#x02014; altered the perceived prevalence and even the definition of the disease. By the mid-1920s, rickets was promoted as universal, at times invisible, to non-experts, but present to some degree in nearly every young child regardless of race or class. It was thus used to promote the young disciplines of preventive medicine, pediatrics, and public health.</p><p>Rickets therefore provides an excellent window into the early politics of preventive health in the United States and a relevant historical counterpoint for current debates over the role of race and ethnicity as risk factors for disease, the use of diagnostic technology in defining disease, and the promotion of targeted interventions for today&#x02019;s so-called &#x0201c;lifestyle&#x0201d; diseases.</p></body></sub-article><sub-article id="d31e78" article-type="abstract"><front-stub><title-group><article-title>Providing Providers: Abortion Training for Physicians in the United States, 1920-2007</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ayres</surname><given-names>Soledad T.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Rogers</surname><given-names>Naomi</given-names></name></contrib></contrib-group><aff>Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>This work was designed to investigate the teaching of induced abortion to allopathic medical doctors in 20th-century United States. Elective termination of pregnancy is an extremely common procedure in the United States. While abortions have been and continue to be performed by nurses and midwives as well as by physicians, the training of medical doctors is of particular interest. Their lengthy formal training and historical stature as a highly educated group have garnered a respect in the public eye and an image as safe and knowledgeable providers, even where abortion training might have been lacking. This project aimed to determine the exposure of medical students and residents to abortion procedures in their routine course of training.</p><p>A literature search was conducted, including journal articles, books, and conference proceedings, from 1920 to 2007. Particular attention was paid to reports of medical student and resident didactic and clinical experience with abortion. Resident experience with management of incomplete abortions was considered as an additional source of procedural experience prior to legalization.</p><p>The most surprising finding was that residents might have had greater procedural experience prior to legalization of abortion. In the era of illegal abortion, many women presented to hospitals with incomplete abortions, which were managed using techniques that also could be employed to interrupt a stable pregnancy. Residents, thus, had more procedural training and experience with complications. Once abortion was legalized, these cases dropped dramatically. Since most abortions took place and continue to take place in freestanding outpatient clinics, training physicians have little exposure. So while training in pregnancy-options counseling may be available now where it was previously lacking, the technical skills needed to provide safe and effective terminations may be more difficult for residents to acquire.</p></body></sub-article><sub-article id="d31e105" article-type="abstract"><front-stub><title-group><article-title>Mode of Delivery Decisions among HIV-Infected Mothers at an Urban Maternity Hospital in Nairobi, Kenya</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Beard</surname><given-names>Jessica H.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Ndegwa</surname><given-names>Serah W.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Farquhar</surname><given-names>Carey</given-names></name></contrib><contrib contrib-type="author"><name><surname>Ong&#x02019;ech</surname><given-names>John O.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Mbori-Ngacha</surname><given-names>Dorothy</given-names></name></contrib><contrib contrib-type="author"><name><surname>Govedi</surname><given-names>Fridah</given-names></name></contrib><contrib contrib-type="author"><name><surname>Mutsotso</surname><given-names>Winnie</given-names></name></contrib><contrib contrib-type="author"><name><surname>Kiarie</surname><given-names>James N.</given-names></name></contrib></contrib-group><aff>Department of Obstetrics and Gynaecology, University of Nairobi, Nairobi, Kenya</aff><author-notes><fn><p>Sponsored by Stephen F. Thung, Department of Obstetrics and Gynecology, Yale University School of Medicine</p></fn></author-notes></front-stub><body><p><bold>Purpose:</bold> The objectives of this study are to describe mode of delivery decision making among HIV positive women, understand patient knowledge and attitudes regarding elective cesarean section (ECS) for prevention of mother-to-child transmission of HIV (PMTCT), and, in turn, quantify the use of ECS for PMTCT at an urban Kenyan maternity hospital.</p><p><bold>Methods:</bold> This is a descriptive cross-sectional study involving the survey of postpartum HIV-infected women delivering at Pumwani Maternity Hospital (PMH) in Nairobi, Kenya. Each participant was interviewed using a standardized questionnaire</p><p><bold>Results:</bold> Two hundred fifty women participated in this study over the course of three months. The rate of delivery by ECS for PMTCT was 4.0 percent (10/250), though 13.6 percent (34/250) planned this mode of delivery. Planning ECS was positively correlated with higher education levels (OR: 1.46; 95 percent CI: 1.09-1.94, p = 0.028) and markers of higher socio-economic status, including having a private toilet (OR: 2.89; 95 percent CI: 1.43-3.84, p = 0.002) and living in a home with greater than one room (OR: 2.89; 95 percent CI: 1.07-7.80, p = 0.033). The strongest correlates of ECS planning included having a surgical history (OR = 5.86, 95 percent CI: 2.92-11.77, p &#x0003c; 0.001), attending clinic at PMH (OR = 7.85, 95 percent CI: 4.63-13.30, p &#x0003c; 0.001), and knowledge of ECS (OR = 24.50, 95 percent CI: 8.10-93.35, p &#x0003c; 0.001). Patient education regarding ECS for PMTCT was limited, and 64 percent (160/250) of participants had never heard of this PMTCT intervention. The most often cited concerns regarding cesarean section included increased recovery time (66.3 percent), minor complications (55.4 percent), and risk of death (48.7 percent). Post-counseling, 48.0 percent (120/250) of participants would choose elective cesarean section if offered, while 67.6 percent (169/250) would opt for this mode of delivery if the cost of ECS was the same as vaginal delivery. Correlates of ECS acceptability included high socio-economic status (e.g., secondary education OR = 1.64, 95 percent CI: 1.25-2.15, p &#x0003c; 0.001; ability to pay for delivery OR = 1.40, 95 percent CI: 1.12-1.76, p = 0.003), surgical history (OR = 2.79, 95 percent CI: 1.21-6.43, p = 0.011), and attendance at PMH antenatal clinic (OR = 3.03, 95 percent CI: 1.54-5.98 p = 0.001).</p><p><bold>Conclusions:</bold> Patient knowledge and uptake of ECS for PMTCT is limited at PMH. Although women are aware of the dangers of ECS, post-counseling acceptability of ECS, especially if the burden of cost is removed, is high.</p></body></sub-article><sub-article id="d31e182" article-type="abstract"><front-stub><title-group><article-title>Heart Rate Variability as a Predictor of Hypotension</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Boone</surname><given-names>Louvonia R.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Awad</surname><given-names>Aymen Alian</given-names></name></contrib><contrib contrib-type="author"><name><surname>Galante</surname><given-names>Nicholas</given-names></name></contrib><contrib contrib-type="author"><name><surname>Calo</surname><given-names>Lisbeysi</given-names></name></contrib><contrib contrib-type="author"><name><surname>Shelley</surname><given-names>Kirk</given-names></name></contrib><contrib contrib-type="author"><name><surname>Silverman</surname><given-names>David</given-names></name></contrib></contrib-group><aff>Department of Anesthesiology, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>Heart-rate variability describes beat-to-beat variance. This physiological phenomenon is a marker of a healthy and responsive neuro-cardiovascular system. Indeed, there is a substantive body of research examining the decreased and otherwise altered profiles of older individuals, those with diabetic neuropathy, and patients after infarctions. A new area of interest lies in the use of heart-rate variability as a predictor of acutely adverse outcomes, and one critical area of interest is hypotension. Several researchers have reviewed the effects of spinal anesthesia &#x02014; with its resultant sympathectomy &#x02014; in obstetrical patients utilizing an array of heart rate variability tools. Hanss et al. performed retrospective analysis and identified an LF/HF ratio greater than 2.5 as a significant benchmark for parturients who would develop hypotension after spinal anesthesia. In our institution, we studied a smaller cohort of 26 women and did not find this number or the general trend of greater sympathetic tone in hypotensive women. We subsequently studied the heart-rate variability in controlled research settings using lower body negative pressure to simulate central hypovolemia. We found clear, albeit not statistically significant, separation between subjects who are responders and those who are non-responders. These separations were noted before inter-group differences in heart rate. Our studies show promising, if not significant, separation in a controlled research setting. Considering the great inter-group variability and challenges encountered in obtaining appropriate short-term ECG recordings, we present cautious hope for clinical application.</p></body></sub-article><sub-article id="d31e229" article-type="abstract"><front-stub><title-group><article-title>Evolution of the Surgeon Volume/Patient Outcome Relationship</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Boudourakis</surname><given-names>Leon D.</given-names></name></contrib></contrib-group><aff>Section of Endocrine Surgery, Department of Surgery, Yale University School of Medicine, New Haven, Connecticut</aff><author-notes><fn><p>Sponsored by Julie Ann Sosa and Sanziana A. Roman</p></fn></author-notes></front-stub><body><p>Over the past decade, many studies have shown higher surgeon volume is associated with improved patient outcomes. These findings have prompted informal recommendations for increasing specialization and referrals to high-volume surgeons. To date, the potential impact of these recommendations has not been measured.</p><p>We performed cross-sectional analyses using 1999 and 2005 discharge information from the Healthcare Cost and Utilization Project National Inpatient Sample (HCUP-NIS) to measure whether high-volume surgeon share increased over time for procedures shown in the literature to have strong surgeon volume-outcome associations. Mortality rates (or complications for thyroidectomy) and length of stay (LOS) were examined in order to identify if outcomes are better now when high-volume surgeons perform the procedure compared to low-volume surgeons. The database was searched by ICD-9 procedure and diagnosis codes for colectomy, esophagectomy, gastrectomy, and pancreas resection performed for cancer, coronary artery bypass graft surgery (CABG), carotid endarterectomy (CEA), and thyroidectomy. Definitions of high-volume thresholds were based on a systematic review of the literature.</p><p>High-volume surgeon share increased over a seven-year period for most procedures, as evidence mounted in support of a surgeon volume-outcome association. The most dramatic increase was seen for gastrectomy (106 percent), lobectomy (100 percent), and   esophagectomy (49 percent). There was a decrease in high-volume surgeon share for CABG and CEA, perhaps due to a decline in overall procedure volume, suggesting the potential need to lower the definition of a high-volume threshold.</p><p>For all procedures, the apparent gap in mortality rates (or complications for thyroidectomy) narrowed after adjustment over time between high- and low-volume surgeons. Significant differences in LOS persisted, however. While mortality rates among patients undergoing surgery by a low-volume surgeon have decreased over time, they continue to have longer LOS. This may be related to increased complications and/or less effective post-operative clinical pathways by low-volume surgeons.</p></body></sub-article><sub-article id="d31e256" article-type="abstract"><front-stub><title-group><article-title>Inhibition of Mitochondrial Protein Translation Sensitizes Melanoma Cells to Arsenic Trioxide Cytotoxicity via a Reactive Oxygen Species Dependent Mechanism</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Bowling</surname><given-names>Benjamin D.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Doudican</surname><given-names>Nicole</given-names></name></contrib><contrib contrib-type="author"><name><surname>Manga</surname><given-names>Prashiela</given-names></name></contrib><contrib contrib-type="author"><name><surname>Orlow</surname><given-names>Seth J.</given-names></name></contrib></contrib-group><aff>Department of Dermatology, New York University School of Medicine, New York City, New York</aff><author-notes><fn><p>Sponsored by David J. Leffell, Department of Dermatology, Yale University School of Medicine</p></fn></author-notes></front-stub><body><p>Current standard chemotherapeutic regimens for malignant melanoma are unsatisfactory. Although <italic>in vitro</italic> studies of arsenic trioxide (ATO) have demonstrated promise against melanoma, recent phase II clinical trials have failed to show any significant clinical benefit when used as a single agent. To enhance the efficacy of ATO in the treatment of melanoma, we sought to identify compounds that potentiate the cytotoxic effects of ATO in melanoma cells. Through a screen of 2,000 marketed drugs and naturally occurring compounds and subsequent mechanistic testing, a variety of antibiotic inhibitors of mitochondrial protein translation were identified. The most effective of the agents identified, thiostrepton, significantly enhanced ATO-mediated cytotoxicity and apoptosis in a panel of melanoma cell lines. Treatment with thiostrepton resulted in reduced levels of the mitochondrial-encoded protein cytochrome oxidase I. Exposure to thiostrepton in combination with ATO resulted in a dramatic increase in cellular levels of reactive oxygen species (ROS). Furthermore, addition of the free radical scavenger N-acetyl-l-cysteine (NAC) rescued cells from ATO/thiostrepton-mediated cytotoxicity. Our data suggest that thiostrepton enhances the cytotoxic effects of ATO through a ROS-dependent mechanism. Co-administration of oxidative stress-inducing drugs such as thiostrepton in order to enhance the efficacy of ATO warrants further clinical investigation.</p></body></sub-article><sub-article id="d31e298" article-type="abstract"><front-stub><title-group><article-title>Molecular Genetic Approaches to Diseases of Neural Development</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Bydon</surname><given-names>Mohamad</given-names></name></contrib><contrib contrib-type="author"><name><surname>Bilguvar</surname><given-names>Kaya</given-names></name></contrib><contrib contrib-type="author"><name><surname>Bayrakli</surname><given-names>Fatih</given-names></name></contrib><contrib contrib-type="author"><name><surname>Kolb</surname><given-names>Luis</given-names></name></contrib><contrib contrib-type="author"><name><surname>Gunel</surname><given-names>Murat</given-names></name></contrib></contrib-group><aff>Department of Neurosurgery, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>This study utilized novel genetic techniques in order to find causative gene mutations that underlie diseases of neural development. Our laboratory has collected 175 cases of malformations of cortical development (MCD) from the United States and Europe. Four of these cases are the focus of this manuscript: two familial cases of infantile neuroaxonal dystrophy (INAD), a familial case of hereditary spastic paraparesis (HSP), and a sporadic case of Greig cephalopolysyndactyly (GCPS) and cerebral cavernous malformations (CCMs). The techniques utilized to study the affected patients include microarray-based single nucleotide polymorphism (SNP) genotyping and copy number variation (CNV) analysis, both of which are powerful tools in the hunt for disease-causing gene mutations. In the familial cases of INAD, we report two novel mutations in the <italic>PLA2G6</italic> gene, previously shown to cause INAD when mutated. In the familial case of HSP, we demonstrate linkage to the SPG11 locus on chromosome 15q. Finally, in the sporadic case of GCPS and CCM, we published the first report on this novel syndrome along with a genetic analysis that demonstrates a microdeletion on chromosome 7p, resulting in heterozygous loss of both the <italic>GLI3</italic> and <italic>CCM2</italic> genes. The three studies presented in this manuscript demonstrate the utility of SNP genotyping and CNV analysis in revealing the genetic mutations that underlie diseases of neural development.</p></body></sub-article><sub-article id="d31e348" article-type="abstract"><front-stub><title-group><article-title>Early to Bed: Psychosocial Predictors of Sexual Initiation and High Risk Sexual Behaviors in Early Adolescence</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Caminis</surname><given-names>Argyro P.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Henrich</surname><given-names>Chris H.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Ruchkin</surname><given-names>Vladislav</given-names></name></contrib><contrib contrib-type="author"><name><surname>Schwab-Stone</surname><given-names>Mary E.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Martin</surname><given-names>Andr&#x000e9;</given-names></name></contrib></contrib-group><aff>Yale Child Study Center, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>This thesis examines psychosocial factors associated with risky sexual behavior in early adolescence in order to inform the development of adolescent sexual risk-reduction programs.</p><p>Through a longitudinal study, data were collected through a self-report survey, the Social and Health Assessment (SAHA), which was administered in three waves between 2001 and 2003 to a cohort of incoming sixth-graders in the public school system (149 classes at 17 middle and high schools, N = 1,175) of a small northeastern city in the United States.</p><p>We first examined whether internalizing and externalizing problems in sixth grade, and the rate of change in these factors during middle school, were predictive of sexual initiation two years later, when most of the sample was in eighth grade. We then assessed whether internalizing and externalizing problems in sixth grade, and the rate of change in these factors during middle school, were predictive of engaging in high-risk sexual behavior over the subsequent two years.</p><p>Externalizing factors are more predictive of sexual risk in early adolescence than are internalizing factors. Specifically, substance use and violent delinquency over the course of middle school were associated with higher sexual initiation rates during middle school, while anxiety was associated with lower rates. Additionally, increased substance use over the course of middle school was associated with greater likelihood of engaging in high-risk sexual behavior.</p><p>By identifying particular psychosocial risk factors among young adolescents, the findings of this study support the design of comprehensive youth development programs for high-risk middle school students as a means of reducing health-compromising sexual activity among young teens.</p></body></sub-article><sub-article id="d31e397" article-type="abstract"><front-stub><title-group><article-title>Vertebral Artery Elongation During Whiplash Trauma</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Carlson</surname><given-names>Erik J.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Ivancic</surname><given-names>Paul</given-names></name></contrib><contrib contrib-type="author"><name><surname>Tominaga</surname><given-names>Yasuhiro</given-names></name></contrib><contrib contrib-type="author"><name><surname>Ito</surname><given-names>Shigeki</given-names></name></contrib><contrib contrib-type="author"><name><surname>Rubin</surname><given-names>Wolfgang</given-names></name></contrib><contrib contrib-type="author"><name><surname>Panjabi</surname><given-names>Manohar</given-names></name></contrib></contrib-group><aff>Biomechanics Research Laboratory, Department of Orthopaedics and Rehabilitation, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>Elongation-induced vertebral artery (VA) injury has been hypothesized to occur during non-physiologically coupled head motions during automobile impacts. Although previous work has investigated VA elongation during head-forward rear impacts, no studies have performed similar investigations for head-turned rear, frontal, or side impacts. The present study comprehensively quantifies dynamic VA elongations during simulated head-forward rear, head-turned rear, frontal, and side impact automotive collisions, and compares these data among impact configurations and with corresponding physiological limits. A biofidelic whole-cervical spine model with muscle force replication and surrogate head underwent simulated frontal impacts (n = 6) of 4, 6, 8, and 10 g or head-forward rear (n = 6), head-turned rear (n = 6), and left side (n = 6) impacts of and 3.5, 5, 6.5, and 8 g. Significant increases (P &#x0003c; 0.05) in average (SD) peak dynamic VA elongation above physiological of up to 30.5 (2.6) mm during head-turned rear impact and 17.4 (2.6) mm during side impact were observed beginning at 5 g and 6.5 g, respectively. Highest peak elongation of 5.8 (2.1) mm during head-forward rear impact and 2.5 (2.4) mm during frontal impact did not exceed physiological limit. The times of occurrence of peak VA elongation were earlier and rates later during head-turned rear and side impact vs. head-forward rear and frontal impact. Results indicate that head-turned rear and side impacts may cause elongation-induced VA injury leading to chronic symptoms of vascular insufficiency reported by whiplash patients. Elongation-induced VA injury during head-forward rear and frontal impacts is unlikely.</p></body></sub-article><sub-article id="d31e444" article-type="abstract"><front-stub><title-group><article-title>Visual Acuity and Central Macular Thickness in Patients Treated with Anti-Angiogenic Intravitreal Injections for Age-Related Macular Degeneration</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Castiblanco</surname><given-names>Claudia P.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Adelman</surname><given-names>Ron A.</given-names></name></contrib></contrib-group><aff>Department of Ophthalmology, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>To study central macular thickness (CMT) and visual acuity (VA) changes in a series of patients with age-related macular degeneration (AMD) treated with ranibizumab alone vs. those treated in combination with bevacizumab, pegaptanib sodium, or photodynamic therapy (PDT). Retrospective study of patients treated at the Yale Eye Center. CMT and VA measurements were recorded at the initial injection of ranibizumab and at one, three, six, nine, and 12 months. Fifty-eight eyes of 53 patients received ranibizumab therapy. They had a mean initial VA of 20/320 (logMAR 1.2) and CMT of 294 &#x000b5;m. At 12 months, the VA was 20/250 (logMAR 1.1) and CMT decreased to 255 &#x000b5;m. A mean of six ranibizumab injections/year were given, and the mean follow-up was 10.2 months. Injections were given on a monthly basis for the first three months and then as needed based on optical coherence tomography (OCT). Twenty-four of these eyes received ranibizumab as the only anti-angiogenic agent, 18 eyes had prior pegaptanib injections to ranibizumab, eight eyes had prior bevacizumab injections and eight patients had pegaptanib and bevacizumab prior to starting ranibizumab. Eight eyes had pegaptanib only, and 61 eyes had PDT only. Patients who received ranibizumab alone experienced stabilization of VA and CMT decreased significantly at three- and 12-month follow-up. Patients treated with pegaptanib and bevacizumab prior to ranibizumab demonstrated a similar response. OCT is useful in following therapy response and determining the need for re-injection.</p></body></sub-article><sub-article id="d31e468" article-type="abstract"><front-stub><title-group><article-title>Seizure Awareness Among Epilepsy Patients</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Correll</surname><given-names>Cynthia M.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Spencer</surname><given-names>Susan S.</given-names></name></contrib></contrib-group><aff>Department of Neurology, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>We examined patients&#x02019; seizure awareness as it relates to various seizure characteristics in order to define a subset of epilepsy patients at increased risk for not recognizing their seizures. This study consisted of a retrospective chart/video review component and prospective inpatient seizure monitoring component with patients&#x02019; seizure awareness determined by video or direct patient report, respectively. Seizure awareness was related to seizure location, subtype, vigilance state, aura, etiology, neuropsychological scores, patients&#x02019; self-reports of awareness, and seizure awareness as charted by the physician. The 37 patients in the retrospective component and 29 patients in the prospective component failed to recognize 39 percent and 44 percent of their seizures, respectively. We also revealed a number of seizure characteristics that significantly related to decreased seizure awareness (all p &#x0003c; 0.02). Temporal lobe seizures were more likely to go unrecognized (48 to 62 percent) than extratemporal (24 to 27 percent), as were left or dominant hemisphere seizures (55 to 59 percent) compared to right or nondominant hemisphere seizures (20 to 30 percent). Left or dominant temporal lobe seizures were much more likely to be unrecognized (66 to 77 percent) than right or nondominant (12 to 37 percent). Only CPS (63 percent) and GTC (46 percent) seizures went unrecognized, while SPS (0 percent) never did. Seizures during sleep were unrecognized (55 to 56 percent) more often than seizures during wake state (26 to 33 percent). Akin to the observation in seizures types, seizures without aura were unrecognized (54 to 64 percent) considerably more often than seizures with aura (3 to 7 percent). Seizures caused by infectious (78 percent) or traumatic (88 percent) etiology were more often unrecognized than idiopathic seizures (42 percent) or seizures caused by structural etiologies (4 percent). Our study also highlights the patients&#x02019; and physicians&#x02019; inabilities to determine a patient&#x02019;s seizure awareness level, with only 67 percent of patients and 47 percent of physicians correctly identifying the patient&#x02019;s seizure awareness level. Overall, we conclude that subsets of patients with any or all the of following characteristics are more likely to have decreased seizure awareness: temporal lobe seizures, left or dominant hemisphere seizures, left or dominant temporal lobe seizures, CPS or GTC seizures, seizures during sleep, seizures without aura, and seizures caused by infection or trauma. We hope that by using these characteristics physicians will be able to more accurately determine those patients who are at higher risk of having unrecognized seizures so that more intensive seizure monitoring can be provided as needed.</p></body></sub-article><sub-article id="d31e491" article-type="abstract"><front-stub><title-group><article-title>The Role of Amp-Dependent Protein Kinase in Acute Pancreatitis</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Diaz de Villalvilla</surname><given-names>Alexander P.E.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Thrower</surname><given-names>Edwin C.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Gorelick</surname><given-names>Fred S.</given-names></name></contrib></contrib-group><aff>Department of Internal Medicine, Section of Digestive Diseases, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>Acute pancreatitis (AP) is an inflammatory disease of the pancreas involving premature activation of digestive enzyme zymogens. Since energy stress and ischemia are important in the development of AP, we studied the role of a cellular energy-sensing enzyme, AMP-dependent protein kinase (AMPK), in groups of pancreatic acinar cells &#x02014; acini &#x02014; which have phenotypic responses that match those of early pancreatitis. We used the cholecystokinin orthologue caerulein to stimulate acini and induce experimental pancreatitis. Using this system, activation of AMPK with the agonist AICAR decreased zymogen activation, while inhibition with the antagonist compound C enhanced it; both AMPK modulators appeared to influence a later stage of zymogen activation. Stimulation of acini with physiologic concentrations of caerulein caused mild changes in AMPK activity. However, when exposed to hyperstimulatory doses that caused pancreatitis, caerulein caused a rapid and prolonged decrease in AMPK activity. Unexpectedly, AMPK activity varied inversely with protease activation. The hyperstimulation-induced change in AMPK activity was accompanied by modifications in phosphorylation at the enzyme&#x02019;s activating and inhibitory sites. Further, a transient decrease in AMPK protein levels was observed with hyperstimulation.</p><p>We investigated the processes involved in zymogen activation by developing a cell-free assay using zymogen-containing compartments and cytosol. We found that cytosolic factors were necessary to cause zymogen activation, and adenosine nucleotides tended to inhibit trypsinogen and enhance chymotrypsinogen activation. Time-course studies of secretion and cell injury in acini in the presence of AICAR and compound C revealed no effects of modulating AMPK on either process.</p><p>These findings suggest that AMPK activity is linked to early pathologic protease activation in the pancreatic acinar cell. The cellular targets of AMPK will be the subjects of future studies.</p></body></sub-article><sub-article id="d31e524" article-type="abstract"><front-stub><title-group><article-title>The Role of Matrix Metalloproteinases in Axon Guidance and Neurite Outgrowth</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Dinglasan</surname><given-names>Lu Anne Velayo</given-names></name></contrib><contrib contrib-type="author"><name><surname>Greer</surname><given-names>Charles</given-names></name></contrib><contrib contrib-type="author"><name><surname>Treloar</surname><given-names>Helen</given-names></name></contrib></contrib-group><aff>Department of Neurosurgery, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>Axons navigating the complex environment of the developing central nervous system (CNS) use extracellular guidance cues to help find their correct synaptic target. Matrix metalloproteinases (MMPs), a family of zinc-dependent proteolytic enzymes, have been shown to regulate axon guidance by degrading extracellular matrix (ECM) or by cleaving guidance cues and their receptors. The olfactory system is an excellent model for studying the role of MMPs in axon guidance because of its capacity for continuous nerve regeneration and topographic maintenance during synaptic targeting. I hypothesized that MMPs may play a role in guiding olfactory sensory neurons to their correct glomerular target by sculpting the ECM and influencing axon interactions with the environment. To investigate this, I used RT-PCR to screen 19 members of the MMP family and their four endogenous inhibitors (tissue inhibitors of metalloproteinases, TIMPs) and performed immunohistochemistry to localize candidate MMP and TIMP proteins. Two MMP sub-families, the gelatinases and the membrane-bound MMPs (MT-MMPs), showed distinctive spatio-temporal expression patterns across different stages of olfactory development, consistent with a role in axon pathway formation. To assess gelatinases in their active form, I performed <italic>in situ</italic> zymography and found restricted patterns of proteolytic activity within the developing olfactory nerve. Finally, to study the role of MMPs in pathway formation, I applied active recombinant MT-MMPs to common ECM molecules found in the developing olfactory system, such as tenascin and proteoglycans, and examined subsequent changes in neurite outgrowth. The inhibitory effects of these substrata were decreased with enzyme treatment, with MT-MMPs having different substrate specificities and degradation efficiencies that allow for increased neurite outgrowth in culture. Collectively, the data suggest that MMPs are active in the developing olfactory system and have a role in axon guidance and neuronal pathway formation.</p></body></sub-article><sub-article id="d31e556" article-type="abstract"><front-stub><title-group><article-title>The Impact of Pregnancy and Childbirth on Gamma-Aminobutyric Acid Neuronal Function</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Dominguez</surname><given-names>Jennifer Estrella</given-names></name></contrib><contrib contrib-type="author"><name><surname>Epperson</surname><given-names>C. Neill</given-names></name></contrib></contrib-group><aff>Department of Psychiatry, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>Our present knowledge of the neuroendocrine changes associated with pregnancy and parturition is based primarily upon peripheral measures of ovarian hormones. Despite numerous investigations from productive laboratories, the impact of these dramatic hormonal fluctuations on brain function and their role in mediating normal maternal behavior or common postpartum mood disturbances has not been well elucidated. We hypothesized that occipital cortical gamma-aminobutryic acid (GABA) concentrations would be reduced in healthy, postpartum women compared to regularly menstruating, non-postpartum women in the follicular phase of their menstrual cycle, and that a gradual increasing trend in GABA would be seen across the puerperium. We also hypothesized that occipital cortical GABA concentrations would differ between non-depressed postpartum women with a personal or family history of a mood or anxiety disorder compared to healthy, parturient women with no such history. We used the novel neuroimaging technology, proton magnetic resonance spectroscopy (&#x000b9;H-MRS), to investigate the effect of the early and extended puerperium in healthy, postpartum women (n = 12), as well as &#x0201c;at-risk&#x0201d; postpartum women (n = 5) with a personal or family history of mood or anxiety disorder, on occipital cortex GABA concentrations. We compared these measurements with those from healthy, non-puerperal women (n = 14) in the follicular phase of their menstrual cycle. Compared to follicular phase controls, we found no significant differences in GABA concentrations in the early postpartum period in either healthy, postpartum women or &#x0201c;at-risk&#x0201d; postpartum women. GABA concentrations remained stable over time in each group. There was a trend toward greater variation in GABA over time in the &#x0201c;at-risk&#x0201d; group compared to follicular controls; however, this did not reach statistical significance (<italic>t = -1.84, df = 13, p = 0.088</italic>), and these findings must be interpreted cautiously, due to the small sample size with repeated measures in the &#x0201c;at-risk&#x0201d; group (n = 3). Contrary to our hypotheses, this study does not support a difference in GABA concentrations measured in the occipital cortex using &#x000b9;H-MRS between healthy or &#x0201c;at-risk&#x0201d; postpartum women and follicular phase, healthy controls. It does show a trend, however, suggesting that further studies may be valuable in exploring the possibility that &#x0201c;at-risk&#x0201d; postpartum women exhibit greater GABA dysregulation.</p></body></sub-article><sub-article id="d31e582" article-type="abstract"><front-stub><title-group><article-title>Clinical and Demographic Predictors of Short-Term Recovery from Arthroscopic Partial Meniscectomy</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Fabricant</surname><given-names>Peter D.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Jokl</surname><given-names>Peter</given-names></name></contrib></contrib-group><aff>Department of Orthopaedics and Rehabilitation, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>Patients undergoing orthopaedic surgery are concerned with returning to activities of daily living (recovery) in addition to the long-term result of their surgery (end result). As evidence of predictors of rate of short-term recovery is limited to date, this study seeks to determine which patient clinical and demographic factors can serve as prognostic indicators for rate of short-term recovery from arthroscopic partial meniscectomy in the year following surgery and how they may differ from previously published associations with long-term outcome. Clinical (depth of meniscal excision, involvement of one or both menisci, extent of meniscal tear, extent of osteoarthritis) and demographic (age, gender, and BMI) measurements were obtained pre- and intraoperatively. Mixed model repeated measures analyses were used longitudinally to identify independent predictors of rate of recovery, measured by prospectively assessing knee pain, knee function, and overall physical knee status preoperatively and at regular intervals throughout postoperative recovery out to one year. Of the clinical variables, only greater extent of osteoarthritis was associated with slower rate of recovery over all three recovery measures. Greater depth of meniscal excision was associated only with poorer overall physical knee status, but not postoperative knee pain or function. Of the demographic predictor variables, female gender was associated with poorer scores over all three recovery variables over time, while age and body mass index (BMI) had no association with rate of recovery. Factors affecting short-term rate of recovery are different than associations with long-term outcome. Previous research has shown poorer long-term outcome with advanced age, greater BMI, and greater amount of meniscal tissue excision. This research indicates that female gender and worse osteoarthritis at the time of surgery are associated with a slower rate of short-term recovery from arthroscopic partial meniscectomy, while age, obesity, and amount of meniscal tear/resection show no association with recovery scores over time throughout the first year postoperatively.</p></body></sub-article><sub-article id="d31e605" article-type="abstract"><front-stub><title-group><article-title>Understanding the Relationship between Women&#x02019;s Participation and Health in Uttar Pradesh, India</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Feldman</surname><given-names>Candace H.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Darmstadt</surname><given-names>Gary L.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Kumar</surname><given-names>Vishwajeet</given-names></name></contrib><contrib contrib-type="author"><name><surname>Ruger</surname><given-names>Jennifer P.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Cappello</surname><given-names>Michael</given-names></name></contrib></contrib-group><aff>Departments of Pediatrics and Epidemiology and Public Health, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>The purpose of this qualitative research study was to better understand perceptions of the limitations, motivations, and influence of women&#x02019;s political participation on the health of a community in northern India. This study was nested within a larger community-based participatory neonatal health intervention led by the <italic>Saksham</italic> study group. Eighteen small focus groups were held in the rural villages of Shivgarh, separated based on gender, age category, and parent study intervention status. Scenarios were presented on culturally sensitive, locally relevant topics surrounding the concept of women&#x02019;s health agency, defined generally as a woman&#x02019;s ability to advocate for better health. Qualitative results were analyzed based on four key discussion themes: participation, autonomy, agency/self-efficacy, and health systems. Elder women were found to demonstrate the greatest sense of self-efficacy, and as a group cited the largest number of successful health advocacy efforts. Women consistently prioritized issues relating to education, child health, and familial well-being. Male concerns included infrastructure repair, village development, and need for business opportunities. Caste was a significant factor in that the greatest political party participation, and sense of self-efficacy, were seen among the highest and lowest caste members, and the strictest limitations to autonomy were among members of the warrior caste. Participation in the community-based intervention had varying effects, showing some differences in self-efficacy but rare improvements in participation, autonomy, and the functioning of the health system. Conclusions include the need to keenly understand the local infrastructure and health system, cultural norms surrounding autonomy, and male and female perceptions of participation and self-efficacy to appropriately define and ultimately improve women&#x02019;s health agency. In addition, in order for a community-based participatory health intervention to truly improve women&#x02019;s empowerment, explicit strategies in keeping with this aim must be as central as the health-related goals.</p></body></sub-article><sub-article id="d31e649" article-type="abstract"><front-stub><title-group><article-title>Relationship of Serum S100B and Intracranial Injury in Children with Accidental Closed Head Trauma</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Frasure</surname><given-names>Sarah Elisabeth</given-names></name></contrib></contrib-group><aff>Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>S100B, a calcium-dependent protein produced by astroglial cells in the central nervous system (CNS) and chondrocytes, functions as both a neurotrophin and neurotoxin. It has a half-life in the systemic circulation of approximately six hours. We examined whether serum levels of S100B would be predictive of intracranial injury (ICI), as detected by cranial computed tomography (CT), in children with closed head trauma (CHT). In addition, we evaluated the effect of long bone fractures on the level of S100B in children with both CHT and extracranial injuries such as long bone fractures. We prospectively enrolled 152 children who presented to the Pediatric Emergency Department of Yale-New Haven Children&#x02019;s Hospital within six hours of accidental CHT and required CT to exclude ICI. After informed consent from a caregiver, samples were obtained by venipuncture and analyzed for a quantitative serum level of S100B. Of the 152 children in this study, 24 had an ICI. Mean S100B levels were significantly greater in children with ICI (0.212 &#x000b5;g/L vs. 0.084 &#x000b5;g/L; p &#x0003c; 0.001), with long bone fractures (0.220 &#x000b5;g/L vs. 0.083 &#x000b5;g/L; p &#x0003c; 0.001), and in children who were non-white (0.127 &#x000b5;g/L vs. 0.081 &#x000b5;g/L; p = 0.03). Sixty-two percent of children with ICI had venipuncture performed more than 120 minutes after head injury. After controlling for time of venipuncture, fractures, and race, mean S100B levels were still greater in children with ICI (0.409 &#x000b5;g/L vs. 0.118 &#x000b5;g/L; p &#x0003c; 0.001). The discriminatory value of S100B to detect ICI, as determined by the area under the receiver operator characteristics (ROC) curve, was 0.67. Further study of S100B is necessary to determine whether this biochemical marker could serve as a useful adjunct in the evaluation of children with CHT.</p></body></sub-article><sub-article id="d31e666" article-type="abstract"><front-stub><title-group><article-title>Physiologic Effect of Relaxation Therapies on Autonomic Tone Early after Acute Coronary Syndromes</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Friedman</surname><given-names>Rachel S.C.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Burg</surname><given-names>Matthew</given-names></name></contrib><contrib contrib-type="author"><name><surname>Lampert</surname><given-names>Rachel J.</given-names></name></contrib></contrib-group><aff>Section of Cardiology, Department of Internal Medicine, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>Post-myocardial infarction (MI) patients are at increased risk of arrhythmic sudden death. Stress and sympathetic activation are known to influence arrhythmogenesis. While relaxation therapies improve psychological well-being in multiple medical illnesses, whether these therapies can positively influence sympathovagal balance in the post-MI population is unknown. We explored the physiologic effects of Reiki, a light-touch relaxation therapy, and music on post-acute coronary syndrome (ACS) inpatients, using heart rate variability (HRV) to assess changes in cardiac autonomic function during treatment. Forty-eight patients with ACS within the last 72 hours were randomized to receive a single 20-minute session of either Reiki, classical music, or a control &#x0201c;minimal distraction environment.&#x0201d; All subjects underwent ambulatory electrocardiogram (ECG) Holter monitoring. Emotional state was assessed by the Likert scale. HRV was analyzed by spectral analysis via fast Fourier transformation during the baseline, intervention, and post-intervention periods and high-frequency power (log-normalized) compared via ANOVA with repeated measures. Adequate Holters were recorded in 12 control, 13 music, and 12 Reiki patients. High frequency (HF) component of HRV, an index of parasympathetic tone, increased significantly during Reiki (0.58 &#x000b1; 0.16) but not during music (-0.1 &#x000b1; 0.16) or control (0.06 &#x000b1; 0.16). The RR interval increased significantly with Reiki and control, but not with music. Reiki significantly reduced reported anxiety and increased the sense of relaxation compared to control (p = 0.04), whereas music did not. In conclusion, post-MI recipients of light-touch from nurses trained in Reiki experienced increased vagal activity and decreased anxiety. Whether longer-term use of this therapy can improve outcomes requires further study.</p></body></sub-article><sub-article id="d31e695" article-type="abstract"><front-stub><title-group><article-title>Chorionic Villus Sampling and the Risk of Hypertensive Disorders Of Pregnancy: A Case Control Study</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ghazal</surname><given-names>Sanaz</given-names></name></contrib><contrib contrib-type="author"><name><surname>Bahtiyar</surname><given-names>Mert O.</given-names></name></contrib></contrib-group><aff>Section of Maternal-Fetal Medicine, Department of Obstetrics, Gynecology, and Reproductive Sciences, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>The purpose of this study was to determine whether there is an association between chorionic villus sampling for prenatal diagnosis and the development of hypertensive disorders of pregnancy. This study is a single-site retrospective case control study using medical records of patients seen at Yale-New Haven Hospital. A total of 448 patients in three groups (first trimester aneuploidy screening with nuchal translucency assessment, genetic amniocentesis, and chorionic villus sampling) were included, and data on maternal characteristics, delivery outcomes, risk factors, and hypertensive outcomes were recorded. Unadjusted odds ratios and odds ratios adjusted for maternal age and race were calculated to compare the probability of gestational hypertension and preeclampsia between the groups, using the nuchal translucency group as the control. In the genetic amniocentesis group, the adjusted odds ratio for gestational hypertension was 1.9 (95 percent CI 0.2-170.1) and the ratio for preeclampsia was 1.4 (95 percent CI 0.19-5.80), both statistically not significant. In the chorionic villus sampling group, the adjusted odds ratio for gestational hypertension was 0.4 (95 percent CI 0.03-4.7) and the ratio for preeclampsia was 0.93 (95 percent CI 0.8-1.07), again both statistically not significant. This study concluded that there is no association between chorionic villus sampling and the development of hypertensive disorders of pregnancy.</p></body></sub-article><sub-article id="d31e718" article-type="abstract"><front-stub><title-group><article-title>An Evaluation of the Therapeutic Trends in Glaucoma and the Potential Impact of Improved Glaucoma Surgery</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Goldberg</surname><given-names>Roger A.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Shields</surname><given-names>M. Bruce</given-names></name></contrib></contrib-group><aff>Department of Ophthalmology and Visual Sciences, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>Although no validated treatment algorithm has emerged in the management of patients with open angle glaucoma (OAG), incisional surgery is typically reserved as a third-line therapy, after medications and laser treatments have failed to control disease progression. However, medications and laser therapies have numerous shortcomings, including high cost, low compliance, and little incremental efficacy over time. The purpose of this thesis is to review the trends in treatment patterns over time and evaluate the impact that a safer, more effective glaucoma operation might have on the therapeutic algorithm for patients with OAG. A multi-pronged approach was used to identify and characterize trends in the management of patients with glaucoma, including an extensive literature search and review. Additionally, leading glaucoma academicians and innovators were interviewed regarding the historical and expected future development of novel incisional approaches to the management of OAG. Then, members of the American Glaucoma Society (AGS) were surveyed online about a hypothetical &#x0201c;ideal&#x0201d; glaucoma drainage device (GDD) procedure with specifically defined metrics for greater efficacy, safety, and ease-of-implantation, compared to current glaucoma surgery. Participants were asked how such an operation would influence their management of patients with OAG. The results were aggregated from an 11-point Likert scale to calculate the mean and standard deviation, and a repeated measures analysis of variance (ANOVA) was performed to determine whether surgeon preferences for the device differed between six patient categories. Post-hoc contrasts were used to assess the significance of responses for each patient category. Overall, the trend in glaucoma surgery over the past decade has shifted toward the use of GDDs and away from a conventional glaucoma filtering procedure, though both still largely remain third-line therapies. In this survey, of the 126 respondents (31 percent of those surveyed), glaucoma specialists indicated that an improved operation would enter into their treatment algorithm in all six clinical settings, especially among those patients who required repeat surgery (mean 86.9 percent; SD 19.4 percent), were being considered for primary surgery (mean 65 percent; SD 29.4 percent), or were uncontrolled on three or more medicines (mean 58.8 percent; SD 31.6 percent). Specialists also felt that they would consider this improved surgery in patients otherwise requiring trabeculoplasty (mean 33.7 percent; SD 27.8 percent) or who were uncontrolled on two medicines (mean 28.9 percent; SD 25.7 percent) or one medicine (mean 10.9 percent; SD 16.5 percent). Repeated measures ANOVA showed a statistically significant surgical preference for this &#x0201c;ideal&#x0201d; surgical procedure between patient categories (overall p-value &#x0003c; 0.001). These findings suggest that glaucoma specialists recommend incisional surgery earlier in the therapeutic algorithm for patients with OAG if an operation is available that is safer, more effective, and easier to perform than current procedures. Continued work to develop and validate such an operation is justified.</p></body></sub-article><sub-article id="d31e741" article-type="abstract"><front-stub><title-group><article-title>Educational and Behavioral Interventions to Reduce Isocyanate Exposure in Auto Body Shops</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Huertas</surname><given-names>Liza Goldman</given-names></name></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Shaoli</given-names></name></contrib><contrib contrib-type="author"><name><surname>Stowe</surname><given-names>Meredith H.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Redlich</surname><given-names>Carrie A.</given-names></name></contrib></contrib-group><aff>Department of Internal Medicine, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>Isocyanates are a major cause of occupational asthma. To reduce worker exposure to isocyanates, we conducted a prospective field intervention study of educational and behavioral feedback interventions. Fourteen auto body shops were randomly assigned to control and intervention groups; 103 workers from these shops consented to study participation. Original surveys of exposure-related &#x0201c;Knowledge and Attitudes&#x0201d; and &#x0201c;Self Reported Behavior&#x0201d; were administered, and behaviors were observed at baseline, six, and 12 months. The intervention group participated in the full intervention with behavioral feedback continued throughout the first six months; the control group had no formal interventions until, at six months, they received educational training alone. In both study groups, knowledge and attitudes related to personal protective equipment and safe work practices improved substantially. Most improvements were sustained at 12 months. The difference in improvement between interventions was borderline significant (p = 0.056), indicating that behavioral feedback could be superior to educational training alone for improving knowledge and attitude scores. For self-reported behavior, greater improvement in the intervention group was not significant (p = 0.15). At baseline, the Self-Reported Behavior score was significantly correlated with Knowledge and Attitudes score and Hispanic ethnicity (p = 0.008, and p = 0.014), but not with job title, group assignment (intervention vs. control), age, or smoking status. Examining correlations at all study periods, group assignment and Knowledge and Attitudes score were both significant variables affecting self-reported behavior, raising the possibility of greater effectiveness of intervention with behavioral feedback. In conclusion, a multi-faceted intervention including educational training and behavioral feedback improves observed and self-reported safety behavior and related knowledge and attitudes in auto body workers exposed to isocyanates. The addition of behavioral feedback generated improvement in overall knowledge and attitudes that was borderline significant. Scores on the Knowledge and Attitudes survey were significantly correlated with self-reported behavior, giving this survey great potential for use in characterizing auto body worker exposure risk and readiness for behavior change.</p></body></sub-article><sub-article id="d31e777" article-type="abstract"><front-stub><title-group><article-title>Central Corneal Thickness in a Puerto Rican Population</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Graeber</surname><given-names>Carolyn P.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Torres</surname><given-names>Marino Blasini</given-names></name></contrib><contrib contrib-type="author"><name><surname>Shields</surname><given-names>M. Bruce</given-names></name></contrib></contrib-group><aff>Department of Ophthalmology, University of Puerto Rico, San Juan, Puerto Rico, and Department of Ophthalmology and Visual Science, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>The aim of this study was to evaluate the mean central corneal thickness in a Puerto Rican population and compare our findings to published mean central corneal thicknesses of Caucasian, Hispanic, and African-American populations in the United States. Volunteers at the Centro M&#x000e9;dico, San Juan, Puerto Rico, completed a survey and participated in an eye exam, which included measurement of intraocular pressure and central corneal thickness. Of 588 Puerto Rican participants, the mean central corneal thickness was 541 &#x000b1; 33 &#x000b5;m, which is significantly thinner than published values in Caucasian populations (P &#x0003c; .002) and of Hispanic populations (P &#x0003c; .03) but thicker than published values for African Americans (P &#x0003c; .05). Among ethnic subgroups in Puerto Rico, no significant differences in mean central corneal thickness were observed. This finding will help clinicians improve diagnosis and management of glaucoma in this population. It also may reflect an increased risk for disease progression and may indicate a need for earlier, more aggressive screening and management among these individuals.</p></body></sub-article><sub-article id="d31e806" article-type="abstract"><front-stub><title-group><article-title>Effect of Robot-Assisted Treadmill Training on Quality of Life for People with Multiple Sclerosis</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Hatcher</surname><given-names>Mary Stillwell</given-names></name></contrib><contrib contrib-type="author"><name><surname>Lo</surname><given-names>Albert</given-names></name></contrib><contrib contrib-type="author"><name><surname>Triche</surname><given-names>Elizabeth</given-names></name></contrib></contrib-group><aff>Department of Neurology, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>The purpose of this pilot crossover study is to describe data on the effect of body weight-supported treadmill training (BWSTT) on quality of life (QoL) for people with multiple sclerosis (MS). Thirteen men and women with gait impairment secondary to MS underwent two blocks of six biweekly BWSTT sessions &#x02014; one block with a robot-driven gait orthotic and one block without &#x02014; with a six-week washout period between the two training blocks. Subjects were stratified as high or low impairment and then randomized to a treatment order: robot-assisted then treadmill alone (RT) or treadmill alone then robot-assisted (TR). Quality of life was assessed before and after both training blocks by the Fatigue Severity Scale (FSS), the 10-scale Multiple Sclerosis Quality of Life Inventory (MSQLI), and a single life satisfaction (LS) item. The effect of treatment type was evaluated for clinical meaningfulness by standardized effect sizes and for statistical significance by repeated measures ANOVA, comparing change during robot-assisted training blocks to change during unassisted training blocks. Additionally, standardized effect sizes and Student t-tests comparing baseline to endpoint data were used evaluate the overall effect on quality of life of both types BWSTT combined. Comparison of the two treatment types showed mixed results with slightly larger effect sizes on the scales favoring robot-assisted training. Overall change from baseline to the end of the second training block showed improvement in all QoL measures. Improvements were significant on scales assessing physical well-being (PCS; p = 0.025), fatigue (MFIS; p = 0.020), pain (PES; p = 0.029), perceived cognitive function (PDQ-5; p = 0.018), and life satisfaction (LS; p = 0.020). While this pilot study was not adequately powered to demonstrate a significant difference between robot-assisted and unassisted BWSTT on quality of life outcomes, the results suggest that robot-assisted BWSTT may provide some advantage over unassisted BWSTT. More generally, this study shows that both types of BWSTT may lead to diverse benefits in quality of life for people with gait function secondary to MS.</p></body></sub-article><sub-article id="d31e835" article-type="abstract"><front-stub><title-group><article-title>Functional Analysis of CCM3: A Gene Contributing to Cerebral Cavernous Malformations</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Hebert</surname><given-names>Ryan Matthew</given-names></name></contrib></contrib-group><aff>Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>Cerebral cavernous malformations (CCM) are a group of homogenous lesions in the brain, spinal cord, and retina that consist of focal sinusoidal dilatations of vasculature and can lead to devastating life-altering and/or life-ending events such as seizure and hemorrhagic stroke. CCM exhibit compromised blood brain barrier due to perturbed endothelial cell to endothelial cell tight junctions, resulting in chronic hemorrhage. Linkage analysis has led to the identification of three loci that segregate to familial CCM (CCM1, CCM2, and CCM3). Previous studies have identified PDCD10 as the gene responsible for CCM3. Recent <italic>in vitro</italic> data have implicated PDCD10 in apoptosis and cell proliferation. Preliminary <italic>in vitro</italic> experiments demonstrate an increase in apoptosis after overexpression of PDCD10. Introduction of mutations found in human CCM into the above expression vector failed to increase apoptosis. HUVECs exposed to conditions of serum deprivation increased expression of CCM3, which preceded an increase in cleaved caspase-3. Conversely, inhibition of PDCD10 expression through siRNA led to a decrease in cell death. These experiments have been helpful in evaluating CCM3 function <italic>in vitro</italic>. To assess the role of CCM3 <italic>in vivo</italic>, we have utilized Cre recombinase-mediated recombination to knock out CCM3 in mice in a tissue-specific manner. The promoters Tie2 and GFAP were used to drive Cre expression in endothelial cells and astrocytes, respectively. Embryos lacking CCM3 in the endothelium exhibited morphology similar to that of CCM1 knockout mice, indicating that CCM3 is important for early vascular development. Mice deficient in CCM3 in astrocytes exhibited dysmorphic features, such as an enlarged brain and smaller body habitus when compared with control mice. Histologically, cortical layering was perturbed in the mutant mice. BrdU assays suggest there is a decrease in the numbers of dividing cells in the superficial layers of the cortex. Analysis of one adult mutant survivor led to the discovery of a CCM present near the cerebellar-pontine angle. Isolation of astrocytes from GFAP mutant mice revealed similar characteristics to our previous siRNA experiments using HUVECs. Mutant astrocytes were resistant to cycloheximide-induced apoptosis as measured by flow cytometry. Our results support the current hypothesis that PDCD10 is a promoter of apoptosis and that astrocytes play a critical role in the formation of CCM.</p></body></sub-article><sub-article id="d31e864" article-type="abstract"><front-stub><title-group><article-title>The Utility of Prophylactic Drainage Following Pancreaticoduodenectomy</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Hochberg</surname><given-names>Abby L.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Salem</surname><given-names>Ronald</given-names></name></contrib></contrib-group><aff>Section of Surgical Oncology, Department of Surgery, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>This study is designed to investigate whether patients who are not drained prophylactically following pancreaticoduodenectomy have rates of complication that are less than or no different from rates of complication for patients who are selectively drained prophylactically following pancreaticoduodenectomy. An additional aim is to compare the overall rates of complication for all patients in this study to the rates of complication following pancreaticoduodenectomy with prophylactic drainage reported in the literature, to determine if non-routine drainage of patients following pancreaticoduodenectomy compromises patient outcome. One hundred six consecutive patients who underwent pancreaticoduodenectomy were included in this study. The first 53 patients were selectively drained following surgery and comprise Group I. The second consecutive 53 patients were not drained following pancreaticoduodenectomy and comprise Group II. Demographic, operative and pathologic factors as well as rates of complication were analyzed. The results show no statistically significant difference between the rates of complication after pancreaticoduodenectomy without prophylactic drainage and the rates of complication after pancreaticoduodenectomy with selective prophylactic drainage. In addition, overall rates of complication after pancreaticoduodenectomy in this study are not greater than rates of complication following pancreaticoduodenectomy with prophylactic drainage reported in the literature. These findings suggest non-routine drainage of patients following pancreaticoduodenectomy does not compromise patient outcome and provide further support for the practice of omitting prophylactic drains following pancreaticoduodenectomy.</p></body></sub-article><sub-article id="d31e887" article-type="abstract"><front-stub><title-group><article-title>Genotypic Confirmation of Transimmunization-Induced Dendritic Cell Maturation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Hoffmann</surname><given-names>Kristin E.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Vasquez</surname><given-names>Gabriel</given-names></name></contrib><contrib contrib-type="author"><name><surname>Berger</surname><given-names>Carole L.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Girardi</surname><given-names>Michael</given-names></name></contrib><contrib contrib-type="author"><name><surname>Tigelaar</surname><given-names>Robert</given-names></name></contrib><contrib contrib-type="author"><name><surname>Edelson</surname><given-names>Richard L.</given-names></name></contrib></contrib-group><aff>Department of Dermatology, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>Transimmunization (TI), a novel modification of the widely used immunotherapy extracorporeal photopheresis (ECP), induces conversion of processed monocytes into cells expressing phenotypic and functional features of dendritic antigen presenting cells (DC). To further characterize TI-induced DC, we analyzed differential gene expression in the monocyte/DC population after TI treatment. Because ECP, the therapy upon which TI is based, has the unique capacity to induce both anti-cancer immune responses in cutaneous T cell lymphoma (CTCL) patients and tolerogenic responses in graft-vs.-host disease (GVHD), we studied TI-induced gene expression changes in both of these patient populations as well as in healthy normal control individuals with the goal of fully characterizing the gene expression profile(s) induced by TI. Peripheral blood leukocytes from six patients (three patients with CTCL and three patients with GVHD) were procured prior to ECP, immediately after ECP, and following TI processing, and were then enriched for monocytes/DC. RNA was extracted and gene expression compared using Affymetrix total human genome microarrays to analyze 39,000 genes. Differential gene expression was considered as a &#x02265; 2-fold change and P-value &#x02264; 0.05. TI induced significant up-regulation of genes associated with DC maturation, including: DC-LAMP, CD80, CD40, and Decysin. In addition, TI induced down-regulation of monocyte genes such as CD33 and CD36. These changes in gene expression were seen in both CTCL and GVHD patients, suggesting that TI is capable of mediating DC differentiation, regardless of disease process. However, some genes (e.g., IL-19, Tryptophan 2,3-dioxygenase) were differentially expressed after TI only in GVHD patients, while others (e.g., heat-shock proteins 70, 27, and 40) were differentially expressed only in CTCL patients. Our microarray findings were confirmed by quantitative real-time PCR on patient samples as well as on samples from healthy normal controls that underwent the TI procedure. Analysis of the microarray data using GeneGo pathway analysis software demonstrated that the chemokines and adhesion signaling pathway were significantly involved in the mechanism of both ECP and TI, suggesting a crucial role for cell adhesion in these therapies. Taken together, our gene expression and pathway data suggest that TI activates specific signaling cascades that lead to activation and up-regulation of mature DC genes. Our results support the use of TI as a method of generating mature dendritic cells for immunotherapy.</p></body></sub-article><sub-article id="d31e934" article-type="abstract"><front-stub><title-group><article-title>A Pilot Study of Changes in Physician Prescribing Practices after Rural Mutual Health Care Implementation in China</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Horng</surname><given-names>Lily M.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Hong</given-names></name></contrib></contrib-group><aff>Yale University School of Public Health, New Haven, Connecticut</aff></front-stub><body><p>In 2002, the Chinese government renewed commitment to rural health. One experimental insurance program, Rural Mutual Health Care (RMHC), provides affordable coverage for rural residents where a previous insurance system, Cooperative Medical System (CMS), was poorly functioning. This study examined how RMHC affected physician prescribing in Fengshan Township, Guizhou Province, China. Six village doctors were chosen for study based on prior reviews showing high, average, or low rates of prescribing errors. Eight hundred fifty-eight prescriptions with the single diagnosis of common cold were systematically sampled from insured and uninsured patient visits between March and May 2003 (under CMS) and 2004 (under RMHC). Peer physicians reviewed prescriptions for inappropriate prescribing. Chi-squared, Fisher&#x02019;s exact, and two-tailed t-tests were used to explore demographic and prescription characteristics. Multiple linear and logistic regressions were used to model outcomes of number of medications, cost, injection use, and inappropriate prescribing with covariates of patient age and gender, prescribing doctor, year, insurance, and year-insurance interaction. Results show mean cost decreased from 13.09 yuan in 2003 to 7.22 yuan in 2004 (p &#x0003c; 0.001). Cost increased from 7.12 yuan for the uninsured to 11.19 yuan for the insured (p &#x0003c; 0.001). After adjusting for other covariates, RMHC had lower drug costs and fewer medications in comparison to CMS (respectively, p = 0.025 and p = 0.001), but RMHC had no significant effect on injection use or inappropriate prescribing (respectively, p = 0.641 and p = 0.912). In conclusion, this study shows RMHC successfully controls medication costs, but likely has little effect on quality of care. A larger, more rigorous study is needed to assess RMHC&#x02019;s impact on quality of care.</p></body></sub-article><sub-article id="d31e957" article-type="abstract"><front-stub><title-group><article-title>Reproductive Desires and Intentions Among HIV-Infected Individuals in Chennai, India</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Houle</surname><given-names>Elizabeth</given-names></name></contrib><contrib contrib-type="author"><name><surname>Cecilia</surname><given-names>Anitha</given-names></name></contrib><contrib contrib-type="author"><name><surname>Pradeep</surname><given-names>Amrose</given-names></name></contrib><contrib contrib-type="author"><name><surname>Prasad</surname><given-names>Lakshmi</given-names></name></contrib><contrib contrib-type="author"><name><surname>Mayer</surname><given-names>Kenneth</given-names></name></contrib><contrib contrib-type="author"><name><surname>Solomon</surname><given-names>Suniti</given-names></name></contrib></contrib-group><aff>YR Gaitonde Center for AIDS Research and Education, Chennai, India</aff><author-notes><fn><p>Sponsored by Brian Forsyth, Department of Pediatrics, Yale University School of Medicine</p></fn></author-notes></front-stub><body><p>Despite the personal and cultural importance of childbearing in India, limited information exists on the extent of childbearing desires and intentions among HIV-infected individuals in this setting. The purpose of this study was to measure the extent of childbearing desires among HIV-infected individuals in Chennai, India, thereby testing the hypothesis that HIV significantly influences the desire to have children. Three hundred HIV-infected individuals were interviewed about childbearing desires and intentions during routine visits for HIV care at an HIV specialty care clinic in Chennai, South India. Sixteen percent of participants expressed desire for childbearing, and 9 percent expressed intention to have children in the future. Desire for children was associated with childlessness (OR 7.38, 95 percent CI 3.18-17.15), longer time since diagnosis with HIV (OR 2.187, 95 percent CI 1.511-5.511), and absence of financial concerns about bearing children (OR 3.81, 95 percent CI 1.77-8.21). Childbearing desires decreased with increasing age (OR 0.922, 95 percent CI 0.87-0.98). Childbearing desires were not associated with measures of disease progression. The most frequently cited concerns about childbearing among participants were the potential of infecting the infant (71 percent) followed by the burden of the participant&#x02019;s own illness (49 percent). Thirty-five percent of participants reported lack of knowledge about reducing transmission of HIV for couples trying to conceive. Although 84 percent of the cohort expressed no desire for childbearing, nearly half (48 percent) of those without desire stated that in the absence of HIV infection they would desire and or intend to have children. When compared with individuals who desired children regardless of HIV infection, these individuals were more inclined to have at least one child already, resided in the state of Andhra Pradesh, had known their diagnosis for a shorter time, and had more childbearing concerns related to HIV infection. Although the prevalence of childbearing desire and intent are lower among this population than in HIV-infected populations studied in other settings, it is likely that childbearing among HIV-infected individuals in India will become increasingly important as HIV-infected patients live longer and healthier lives through increasing access Highly Active Antiretroviral Therapy (HAART) in India.</p></body></sub-article><sub-article id="d31e1008" article-type="abstract"><front-stub><title-group><article-title>The Association between Pediatric Overweight and Ankle Injuries: A Case-Control Analysis</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>House</surname><given-names>Ellen M.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Zonfrillo</surname><given-names>Mark R.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Seiden</surname><given-names>Jeffery A.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Dubrow</surname><given-names>Robert</given-names></name></contrib><contrib contrib-type="author"><name><surname>Baker</surname><given-names>M. Douglas</given-names></name></contrib><contrib contrib-type="author"><name><surname>Spiro</surname><given-names>David M.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Shapiro</surname><given-names>Eugene D.</given-names></name></contrib></contrib-group><aff>Department of Pediatrics, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>Pediatric obesity has become a pandemic, and trauma is the leading cause of pediatric morbidity and mortality. Our hypothesis is that being overweight increases the likelihood of acute ankle injuries in children. Our study consisted of a case-control analysis in an urban pediatric emergency department with subjects between 5 and 19 years of age. Cases were children presenting with acute ankle injury; controls had chief complaints of fever, headache, or sore throat. Demographic information, physical activity score, heights, and weights were obtained on all participants; injury severity was calculated for the cases. Age and gender-specific body mass index percentiles were calculated for all participants, and a multivariate logistic regression was employed to assess the association between overweight and ankle injury. We also examined those ankle injuries that were not enrolled to assess for possible enrollment bias through comparison of weight percentiles and a sensitivity analysis of increasingly more unlikely assumptions. One hundred eighty cases and 180 controls were enrolled. A significant association was observed between overweight and ankle injuries (multivariate-adjusted odds ratio 3.26, 95 percent confidence interval 1.86-5.72, P-value for trend &#x0003c; 0.0001). Due to possible enrollment bias, this result might overestimate the magnitude of the association; however, our sensitivity analysis demonstrated the robustness of the statistical significance of our findings. In summary, overweight children may be at increased risk for acute ankle injuries.</p></body></sub-article><sub-article id="d31e1061" article-type="abstract"><front-stub><title-group><article-title>Condoms Behind Bars: Barriers to Barrier Protection</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Jayasekera</surname><given-names>Rasika</given-names></name></contrib></contrib-group><aff>Section Of History of Medicine, Yale University School of Medicine, New Haven, Connecticut</aff><author-notes><fn><p>Sponsored by John H. Warner</p></fn></author-notes></front-stub><body><p>This study examined the question of whether condom distribution programs should be implemented within all jails and prisons in the United States. Epidemiological data on prisoner HIV prevalence and transmission rates were examined. The arguments for and against condom distribution were examined through the lens of the recent debate regarding a proposal to implement such programs in all state prisons in California. These arguments were then analyzed through existing data on prisoner condom distribution programs. No existing prisoner condom distribution program has been discontinued. Correctional officer safety issue, smuggling of contraband, and frequency of sexual acts within prison seem to be substantially unaffected by existing condom distribution programs. Furthermore, analysis of laws regarding sex in prison in California suggests that the basis of arguments against condom distribution may be legally unfounded. These findings argue that prisoner condom distribution programs should be widely implemented in the United States.</p></body></sub-article><sub-article id="d31e1082" article-type="abstract"><front-stub><title-group><article-title>A Semantic Associate Language Task to Determine Hemispheric Language Dominance with fMRI</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Jindal</surname><given-names>Jenelle A.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Arora</surname><given-names>Jagriti</given-names></name></contrib><contrib contrib-type="author"><name><surname>Schafer</surname><given-names>Robin</given-names></name></contrib><contrib contrib-type="author"><name><surname>Constable</surname><given-names>R. Todd</given-names></name></contrib></contrib-group><aff>Department of Diagnostic Radiology, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>For several decades, the Wada test has remained the gold standard for determining language lateralization in the pre-surgical evaluation of epilepsy. Because of the relative invasiveness of the test, however, alternatives involving functional magnetic resonance imaging (fMRI) are being explored. Critical to these studies is the development of an appropriate task that will activate cortical areas necessary for language processing in subjects. Here we report an fMRI study involving two tasks: a semantic associate task and a word generation task, performed in both epilepsy patients and control subjects. The semantic associate task, first described in detail by McDermott et al., promises to be a robust task for determining language lateralization. We modified this task as previously described to include a recordable response and compared it with the results of the word generation task.</p></body></sub-article><sub-article id="d31e1117" article-type="abstract"><front-stub><title-group><article-title>The Axial Distribution of Lesion-Site Atherosclerotic Plaque Components: An In Vivo Volumetric Intravascular Ultrasound Radiofrequency Analysis of Lumen Stenosis, Necrotic Core, and Vessel Remodeling</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Kaple</surname><given-names>Ryan K.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Mintz</surname><given-names>Gary S.</given-names></name></contrib></contrib-group><aff>Section of Cardiology, New York Presbyterian Hospital, Columbia University, New York, New York</aff><author-notes><fn><p>Sponsored by Raymond R. Russell, Section of Cardiology, Yale University School of Medicine</p></fn></author-notes></front-stub><body><p>Coronary atherosclerosis is an inflammatory process characterized by lipid accumulation in the vessel intima, an overlying fibrous cap, and a variable amount of lipid and necrotic cellular debris at its core. Percutaneous intravascular ultrasound (IVUS) uses a 3F imaging catheter to provide real-time, <italic>in vivo</italic>, cross-sectional images of the arterial wall, lumen, and plaque. Radio frequency analysis of IVUS data characterizes atherosclerotic plaques into necrotic core (NC), dense calcium (DC), fibrofatty (FF), and fibrotic (FI) tissue. We hypothesized that the minimum lumen area (MLA) site will have a different Virtual Histology (VH) IVUS signature than sites proximal or distal. Pre-intervention VH-IVUS was performed in 81 patients (90 de novo lesions: 43 LAD and 47 RCA). Plaque burden, remodeling index, and VH-IVUS plaque composition were assessed throughout the lesion and reference segments as well as at the MLA and maximum (MaxNC) sites. A catheter pullback length of 31.1 &#x000b1; 12.0mm was used to span a lesion length of 13.8 &#x000b1; 9.5mm. The MaxNC site was located at the MLA in 3.3 percent of lesions, proximal to the MLA in 61 percent of lesions (by 4.11mm) and distal to the MLA in 35.6 percent of lesions (by 3.56mm). The %DC was greater at the MaxNC and %FI and %FF plaque were less at the MaxNC than at the MLA site. Lesion fibroatheromas (FAs) were more often detected at the MaxNC than the MLA (96 percent vs. 51 percent) and were more often classified as thin-capped or multilayered than the MLA sites. The remodeling index was greater at the MaxNC than at the MLA sites and correlated with the NC area both at the MLA (r&#x000b2; 0.068, p = 0.013) and at the MaxNC (r&#x000b2; 0.074, p = 0.009). A greater %DC was found in negatively remodeled vessels at the MLA. In summary, grey scale and VH-IVUS show that the site of greatest potential instability (largest NC and remodeling) is rarely at the MLA, but is most often proximal to the MLA. Also, necrotic core on VH is correlated with remodeling index. These in vivo findings are consistent with previously reported histopathologic data and have important implications for the detection and treatment of coronary artery disease.</p></body></sub-article><sub-article id="d31e1148" article-type="abstract"><front-stub><title-group><article-title>The Effect of Teleradiology on Interpretation Times for CT Pulmonary Angiography Studies</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Kennedy</surname><given-names>Scott</given-names></name></contrib><contrib contrib-type="author"><name><surname>Bhargavan</surname><given-names>Mythreyi</given-names></name></contrib><contrib contrib-type="author"><name><surname>Sunshine</surname><given-names>Jonathan H.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Forman</surname><given-names>Howard P.</given-names></name></contrib></contrib-group><aff>Department of Diagnostic Radiology, Yale University School of Medicine, New Haven, Connecticut; Research Department, American College of Radiology, Reston, Virginia</aff></front-stub><body><p>                The purpose of this study was to evaluate the impact of a teleradiology service on the timely interpretation of computed tomography (CT) pulmonary angiography studies. A survey of clinical and imaging physicians was performed to develop achievable goals for interpretation of CT pulmonary angiography studies. Percentages of studies given preliminary written reports within these thresholds were compared for 1,102 pulmonary angiography CT studies from three months before teleradiology was implemented until three months after. Identical control data were matched over the same periods for 1,638 CT brain studies. Data are reported as averages and percentages. Statistical significance was evaluated with two-tailed t-tests. The median of the optimal time for the preliminary written interpretation of a pulmonary angiography CT reported by radiology chairs or their designees was 60 minutes vs. 20 minutes for emergency medicine physicians, who also reported a 40-minute limit for an acceptable interpretation time. There were statistically significant improvements in the percentage of these studies interpreted within 60-minute (51 percent to 62 percent) and 20-minute (9 percent to 13 percent) optimal time thresholds, within the 40-minute acceptable time threshold (34 percent to 43 percent), and in the percentage of studies taking greater than 40 minutes (67 percent to 57 percent). No statistically significant improvement occurred with control CT brain studies. The use of teleradiology to interpret off-hours inpatient imaging can improve imaging study interpretation times. By establishing an agreed-upon time standard for preliminary written reports of such exams, radiologists and treating physicians can collaborate to ensure prompt diagnosis and treatment of potentially lethal illnesses, such as pulmonary embolism.</p></body></sub-article><sub-article id="d31e1183" article-type="abstract"><front-stub><title-group><article-title>Localized Biliary Ischemia in Patients with Hepatic Arteriovenous Malformations, a Newly Recognized Syndrome Occurring in Hereditary Hemorrhagic Telangiectasia: Diagnosis and Management</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Khoury</surname><given-names>Rasha S.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Garcia-Tsao</surname><given-names>Guadalupe</given-names></name></contrib><contrib contrib-type="author"><name><surname>Young</surname><given-names>Lawrence H.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Henderson</surname><given-names>Katharine J.</given-names></name></contrib><contrib contrib-type="author"><name><surname>White Jr.</surname><given-names>Robert I.</given-names></name></contrib></contrib-group><aff>Yale Vascular Malformation Center,, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>The clinical manifestations of hepatic arteriovenous malformations (HAVMs) were elucidated. A review of the literature was undertaken to better understand how HAVMs specifically affect the biliary system. A retrospective review of the 50 patients with HAVMs seen at the Yale University HHT Center was done, including clinical manifestation, intervention and outcome analysis. Of 50 adults with HAVM, median age was 64 (range 17-73) and 84 percent were female. Initially, 74 percent were classified as Type 1, symptomatic heart failure, 16 percent as Type 2, portal hypertension, and 10 percent as Type 3, biliary abnormalities. In Type 1, conversion to Type 3 was associated with the highest mortality and in Type 3, invasive procedures precipitated rapid decline and need for transplant +/- death. Of the Type 3&#x02019;s, case reports of two sisters with localized biliary ischemia were presented. To our knowledge, this is the first description of localized biliary HHT involvement, diagnosis, and management.</p></body></sub-article><sub-article id="d31e1224" article-type="abstract"><front-stub><title-group><article-title>Phase II Multicenter Trial of The Talent&#x02122; Thoracic Endoprosthesis: A Propensity Score Analysis</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Kiguchi</surname><given-names>Misaki</given-names></name></contrib><contrib contrib-type="author"><name><surname>Alexander</surname><given-names>Beaux</given-names></name></contrib><contrib contrib-type="author"><name><surname>Zannetti</surname><given-names>Simona</given-names></name></contrib></contrib-group><aff>Section of Vascular Surgery, Department of Clinical Research, Medtronic Vascular, Santa Rosa, California</aff><author-notes><fn><p>Sponsored by Richard Gusberg, Department of Surgery, Yale University School of Medicine</p></fn></author-notes></front-stub><body><p><bold>Objective</bold>: Endovascular stent-graft repair has great therapeutic potential in treatment of thoracic aortic aneurysms, especially for those high-risk patients who are not optimal candidates for conventional open repair. The Talent&#x02122; Thoracic Stent Graft is one of the first investigational devices to enter phase two trials in the United States, to demonstrate the safety and effectiveness of the device, as compared to other available devices and to conventional open repair. <bold>Methods</bold>: A multicenter, prospective, non-randomized phase two study of the Talent&#x02122; Thoracic Stent Graft was conducted from December 2003 to June 2005 at 38 sites. Standard follow-up evaluations were performed at one month, six months, one year, and annually thereafter. Each follow-up visit included a CT scan, chest X-ray, and physical examination. Primary safety and effectiveness outcomes were measured. Complications were reported as secondary endpoints. Outcomes were summarized by descriptive statistics and hypothesis tests. An additional analysis was performed, comparing all-cause mortality with a retrospective surgical control group derived from several academic centers, using propensity score analysis to allow comparability. <bold>Results</bold>: One hundred ninety-five subjects were enrolled in this IDE clinical investigation, and 194 were implanted with one or more stent graft devices. The Talent&#x02122; Thoracic Stent Graft System test group met its primary safety endpoint with significantly lower all-cause mortality at 12 months of 16.1 percent (31/192, 95 percent CI 11.2 &#x02013; 22.1 percent) compared to the originally approved literature-derived surgical control group rate of 29.8 percent (p &#x0003c; 0.001). An additional <italic>post hoc</italic> surgical control group derived from three academic thoracic surgery practices revealed there was no statistically significant difference between the all-cause mortality rates in the test group and this second control group after propensity score adjustment (odds ratio 1.23, 95 percent CI for OR 0.58 &#x02013; 2.59). The Talent&#x02122; Thoracic Stent Graft System test group met its primary effectiveness endpoint with a successful aneurysm treatment rate of 89.2 percent (116/130, 95 percent CI 82.6 &#x02013; 94.0 percent), exceeding the two-sided 95 percent lower confidence boundary of 80 percent based on prior experience with the device.</p></body></sub-article><sub-article id="d31e1268" article-type="abstract"><front-stub><title-group><article-title>Ultrasound Measurement of the Inferior Vena Cava Diameter in the Assessment of Pediatric Dehydration</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Kim</surname><given-names>Yunie</given-names></name></contrib><contrib contrib-type="author"><name><surname>Santucci</surname><given-names>Karen</given-names></name></contrib><contrib contrib-type="author"><name><surname>Chen</surname><given-names>Lei</given-names></name></contrib></contrib-group><aff>Section of Emergency Medicine, Department of Pediatrics, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>Dehydration is a common pediatric condition, but limitations exist with current methods of assessing pediatric fluid status, particularly with interobserver variabilities in clinical assessment and the inaccuracy and questionable validity of laboratory tests. Bedside ultrasonography (US) measurement of the inferior vena cava (IVC) and aorta (Ao) may be useful in objectively assessing children with suspected dehydration. The objectives of this study were 1) to compare the IVC and Ao diameter (IVC/Ao) ratio of dehydrated children with euvolemic controls and 2) to compare the IVC/Ao ratio before and after intravenous (IV) rehydration in children with clinical dehydration. This prospective observational study was performed in an urban pediatric emergency department. Children between 6 months and 16 years of age with clinical evidence of dehydration and who were to receive IV fluid hydration were enrolled. Bedside US measurements of the IVC and Ao were taken pre- and post-IV fluid hydration administration. An age-, gender-, and weight-matched control was enrolled for each subject. The IVC/Ao ratios of subjects and controls were compared using the Wilcoxon signed rank test, as were the ratios before and after IV hydration for each subject. Thirty-six pairs of subjects and matched controls were enrolled. The IVC/Ao ratios in the subjects were lower as compared with controls (mean of 0.75 vs. 1.01), with a mean difference of 0.26 (95 percent confidence interval: 0.18, 0.35, p &#x0003c; 0.001). In subjects, the IVC/Ao ratios were significantly lower before IV hydration (mean 0.75 vs. 1.09), with a mean difference of 0.34 (95 percent confidence interval: 0.29, 0.39). As measured by bedside US, the IVC/Ao ratio is lower in children clinically assessed to be dehydrated. Furthermore, it increases with administration of IV fluid boluses. The IVC/Ao ratio, as determined by bedside US, is an objective and noninvasive method of evaluating fluid status in children.</p></body></sub-article><sub-article id="d31e1297" article-type="abstract"><front-stub><title-group><article-title>Depression and Resilience during the First Six Months of Internship</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Laff</surname><given-names>Rachel E.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Sen</surname><given-names>Srijan</given-names></name></contrib><contrib contrib-type="author"><name><surname>Guille</surname><given-names>Constance</given-names></name></contrib><contrib contrib-type="author"><name><surname>Southwick</surname><given-names>Steven</given-names></name></contrib></contrib-group><aff>Department of Psychiatry, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p><bold>Purpose</bold>: The goal of this work is to determine the prevalence and severity of depression, the role of resilience upon the development of depression, and the rates of mental health service use among interns during the first six months of internship.</p><p><bold>Methods</bold>: Prior to starting internship, all consented interns completed an online baseline questionnaire assessing demographic characteristics, depression-related history, and resilience. Interns completed online follow-up questionnaires at three months and six months. Depression was assessed with the Patient Health Questionnaire (PHQ-9). Resilience was assessed with the Connor-Davidson Resilience Scale (CD-RISC). Pearson&#x02019;s correlations and logistic regression analyses were used to determine relationships between variables and predictors of depression, respectively.</p><p><bold>Results</bold>: The point prevalence of depression (PHQ score &#x02265; 10) was 2.5 percent at baseline, 24.9 percent at three months and 24.7 percent at six months. The number of interns who expressed passive suicidal ideation was 2.6 percent at baseline, 10.3 percent at three months, and 6.6 percent at six months. Compared to U.S. medical graduates, international medical graduates had a significantly lower prevalence of depression at three months (9.1 percent vs. 25 percent, p = 0.006) and six months (5.9 percent vs. 24.7 percent, p = 0.001). At three months, both a prior history of depression (Odds Ratio (OR) = 3.3, p &#x0003c; 0.02, 95 percent Confidence Interval (CI) = 1.3 &#x02013; 8.7) and a negative response to the CD-RISC item &#x0201c;My past successes give me confidence for new challenges&#x0201d; (OR = 0.32, p &#x0003c; 0.01, CI = 0.17-0.62) were significant predictors of depression. At six months, both female gender (OR = 2.8, p &#x0003c; 0.02, CI = 1.1-6.9) and work hours reported the week prior to follow-up (OR = 1.04, p &#x0003c; 0.01, CI = 1.01 &#x02013; 1.04) were significant predictors of depression. Resilience scores at baseline were significantly correlated with depression scores at all time points; however, they were not predictive of depression at either follow-up point. At three months, only one intern (8.3 percent) with depression scores necessitating treatment (PHQ &#x02265; 15, N = 11) had started some form of treatment. At six months, only one intern (9 percent) with depression scores necessitating treatment (PHQ &#x02265; 15, N = 12) had started some form of treatment.</p><p><bold>Conclusions</bold>: Consistent with prior studies, the prevalence of depression in interns is significantly higher than the general population. There is a significant amount of suicidal ideation among interns during the first three months of internship. It appears that international medical graduates are significantly less depressed than U.S. medical graduates during the first six months of internship. A prior history of depression, longer work hours, female gender, and a lack of confidence in coping with challenges are associated with the development of depression during the first six months of internship. Of greatest concern, very few interns with clinically significant levels of depression are seeking treatment. In the future, it will be important to increase access to mental health services and understand barriers to seeking treatment in order to better address the mental health needs of interns.</p></body></sub-article><sub-article id="d31e1346" article-type="abstract"><front-stub><title-group><article-title>The Regulation of Mitochondrial Uncoupling Proteins in the Heart</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Laskowski</surname><given-names>Karl R.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Volkova</surname><given-names>Maria</given-names></name></contrib><contrib contrib-type="author"><name><surname>Russell</surname><given-names>Raymond R.</given-names></name></contrib></contrib-group><aff>Section of Cardiovascular Medicine, Department of Internal Medicine, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>The mitochondrial uncoupling proteins (UCPs) are a recently discovered group of proteins present in the inner mitochondrial membrane and mediate a variety of important functions. Involved in transmembrane proton transport, UCPs regulate cellular metabolism as well as prevent reactive oxygen species formation (ROS) and detoxify exogenous ROS. In the heart, these proteins may protect tissue during times of ischemic or metabolic stress. Also activated during metabolic stress is adenosine monophosphate-activated protein kinase (AMPK), which has been shown to provide cardioprotection during ischemia/reperfusion. We hypothesized that AMPK activation plays a role in upregulating the expression of uncoupling proteins UCP2 and UCP3 in the heart. Using both tissue and cellular models, we demonstrate that pharmacologic activation of AMPK with an AMP-analogue, 5-aminoimidazole-4-carboxyamide-ribonucleoside (AICAR), leads to increases in UCP2 and UCP3 mRNA and protein expression at both one-hour and 24-hour incubation time points. Furthermore, we identify a segment of the UCP3 promoter that can mediate AMPK-activated transcription. We conclude that AMPK activation appears to induce increased UCP expression, and that such an effect is mediated through an interaction with a specific portion of the UCP3 promoter. These findings support the idea that some degree of the cardioprotective effects observed with AMPK activation may be due to increased UCP expression in the heart.</p></body></sub-article><sub-article id="d31e1375" article-type="abstract"><front-stub><title-group><article-title>Serum S-100B as a Potential Biomarker for Meningitis in Febrile Infants: An Interim Analysis</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Lau</surname><given-names>Kelvin C.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Langhan</surname><given-names>Melissa L.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Bechtel</surname><given-names>Kirsten A.</given-names></name></contrib></contrib-group><aff>Section of Emergency Medicine, Department of Pediatrics, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>The purpose of this study is to identify the utility of serum S-100B levels as a marker for meningitis in febrile infants younger than 3 months.</p><p>All infants younger than 3 months who presented to the Pediatric Emergency Department (PED) of Yale-New Haven Children&#x02019;s Hospital and required both a lumbar puncture and venipuncture due to either a confirmed rectal temperature &#x02265; 38.0&#x000ba; C, as well as those with an overall presentation that concerned the responsible physician for possible meningitis and required a lumbar puncture as part of their PED evaluation, were prospectively enrolled. One hundred eleven patients had participated after a 1.5-year recruitment period or about 40 percent of the 260 subjects calculated a priori to be required over three years in order to achieve 80 percent power. After informed written consent, approximately 1 mL of blood was obtained for the analysis of the serum level of S-100B, in addition to the volume normally drawn for standard laboratory analysis. Patients with confirmed meningitis as defined by a positive viral or bacterial culture, a positive polymerase chain reaction (PCR) for enterovirus or herpes simplex virus, and/or cerebrospinal fluid (CSF) pleocytosis were compared with those subjects without meningitis. S-100B levels for 101 subjects were available for interim analysis, of which 27 (26.7 percent) met the criteria for meningitis and 74 (73.3 percent) did not.</p><p>The median S-100B level in infants with meningitis was 247.0 ng/L (95 percent CI: 103.5, 804), as compared to 199.1 ng/L (142.5, 384.0) in those without meningitis (p &#x0003e; 0.05). Receiver operating characteristic analysis revealed an area under the curve of 0.4917 (0.3940, 0.5863). Ad hoc power calculations demonstrated a 57 percent probability of detecting a difference of 390 ng/L between the two groups when using this sample size.</p><p>At this time, this interim analysis of this ongoing study suggests that a larger sample size still will be required to determine if serum S-100B is a useful marker for meningitis in febrile infants. Because CSF fluid analysis and the associated risks of lumbar puncture remain the only means by which to identify infants with meningitis, the search for a simple serum test to determine the likelihood of meningitis will continue to be worthwhile.</p></body></sub-article><sub-article id="d31e1410"><front-stub><title-group><article-title>Anatomy, Physiology, and Management of Patients with Diffuse Pulmonary Arteriovenous Malformations</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Lazic</surname><given-names>Tamara</given-names></name><xref ref-type="aff" rid="A1">a</xref></contrib><contrib contrib-type="author"><name><surname>Pierucci</surname><given-names>Paola</given-names></name><xref ref-type="aff" rid="A2">b</xref></contrib><contrib contrib-type="author"><name><surname>Henderson</surname><given-names>Katharine J.</given-names></name><xref ref-type="aff" rid="A1">a</xref></contrib><contrib contrib-type="author"><name><surname>White Jr.</surname><given-names>Robert I.</given-names></name><xref ref-type="aff" rid="A1">a</xref></contrib></contrib-group><aff id="A1"><label>a</label>Yale Vascular Malformation Center, Yale School of Medicine, New Haven, Connecticut</aff><aff id="A2"><label>b</label>Department of Respiratory Medicine, Policlinic of Bari, Bari, Italy</aff></front-stub><body><p>Diffuse pulmonary vascular malformations (PAVMs) are a small and understudied, but nevertheless important, subset of the PAVM population of patients, associated with significant mortality and morbidity.</p><p>A review of literature was undertaken to investigate the current understanding of diffuse PAVMs. This review demonstrated that no additional attempts to define diffuse PAVMs and describe their natural history was made before or after the 2000 report by Faughnan and colleagues [<xref ref-type="bibr" rid="R1">1</xref>].</p><p>To further expand the findings from 2000, we performed a retrospective review of 36 patients (21 female, 15 male) with diffuse PAVMs from a cohort of 821 consecutive patients with PAVMs. Diffuse PAVMs were classified angiographically as involving one or more segmental pulmonary arteries in one or both lungs. The following data were noted from the chart review: Hereditary Hemorrhagic Telangiectasia (HHT) status, gender, age at presentation, presence or absence of large focal PAVMs, oxygen saturations, morbidity, and mortality. </p><p>Twenty-nine out of 36 (81 percent) patients had HHT. Diffuse PAVMs were more commonly bilateral 26/36 (72 percent) than unilateral 10/36 (28 percent) (p = 0.02). Initial O&#x02082; saturations of patients with unilateral and bilateral diffuse PAVM were 87 percent &#x000b1; 7 percent and 79 percent &#x000b1; 8 percent (p = 0.02), for which the current values are 95 percent &#x000b1; 3 percent and 85 percent &#x000b1; 7 percent (p &#x0003c; .0001) respectively. Nine deaths occurred, but only in patients with bilateral involvement. Deaths were due to hemoptysis from bronchial artery hypertrophy (2), brain hemorrhage (1) and abscess (1), spontaneous liver necrosis (3), operative death during attempted lung transplantation (1), and hemorrhage from duodenal ulcer (1).</p><p>Yearly follow-up is recommended for this group of patients as they are at high risk for complications.</p></body><back><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faughnan</surname><given-names>ME</given-names></name><name><surname>Lui</surname><given-names>YW</given-names></name><name><surname>Wirth</surname><given-names>JA</given-names></name><name><surname>Pugash</surname><given-names>RA</given-names></name><name><surname>Redelmeier</surname><given-names>DA</given-names></name><name><surname>Hyland</surname><given-names>RH</given-names></name><name><surname>White</surname><given-names>JR Jr.</given-names></name></person-group><article-title>Diffuse pulmonary arteriovenous malformations: characteristics and prognosis</article-title><source>Chest</source><year>2000</year><volume>117</volume><issue>1</issue><fpage>31</fpage><lpage>38</lpage><pub-id pub-id-type="pmid">10631195</pub-id></element-citation></ref></ref-list></back></sub-article><sub-article id="d31e1524" article-type="abstract"><front-stub><title-group><article-title>Long-Term Complications of Septal Dermoplasty in Hereditary Hemorrhagic Telangiectasia Patients</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Levine</surname><given-names>Corinna G.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Henderson</surname><given-names>Katharine J.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Leder</surname><given-names>Steven B.</given-names></name></contrib><contrib contrib-type="author"><name><surname>White Jr.</surname><given-names>Robert I.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Ross</surname><given-names>Douglas A.</given-names></name></contrib></contrib-group><aff>Department of Surgery, Section of Otolaryngology Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p><bold>Objective</bold>: Septal dermoplasty has been recommended as the treatment of choice for life-threatening epistaxis in hereditary hemorrhagic telangiectasia patients. This study evaluates the complications of septal dermoplasty in the management of transfusion-dependent epistaxis.</p><p><bold>Study Design</bold>: Consecutive retrospective study.</p><p><bold>Subjects and Methods</bold>: Between 1994-2006, septal dermoplasty was performed on 106 consecutive patients with transfusion dependent epistaxis. Of 103 potential patients, 37 either died or were lost to follow-up, leaving 66 patients for study. Data on complications and quality of life was collected on 50/66 (76 percent) patients (mean follow-up 3.75 years) via phone interview.</p><p><bold>Results</bold>: At follow-up, 78 percent of patients had experienced nasal odor, 72 percent had nasal crusting, 58 percent had decreased sense of smell, 30 percent noted worsened sinus infections, 88 percent could breathe through their nose, and 86 percent stated improved quality of life.</p><p><bold>Conclusion</bold>: Complications in HHT patients treated with septal dermoplasty for transfusion dependent epistaxis often include nasal odor, nasal crusting, decreased smell, and increased sinus infections. The majority of patients also experience an increase in their quality of life after the surgery.</p></body></sub-article><sub-article id="d31e1583"><front-stub><title-group><article-title>Beyond Patient Satisfaction: Physician Ambivalence, Authenticity, and the Challenges to Patient-Centered Medicine</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Levine</surname><given-names>Kiera</given-names></name><xref ref-type="aff" rid="B1">a</xref></contrib><contrib contrib-type="author"><name><surname>Burt</surname><given-names>Robert A.</given-names></name><xref ref-type="aff" rid="B2">b</xref></contrib><contrib contrib-type="author"><name><surname>Duffy</surname><given-names>Thomas P.</given-names></name><xref ref-type="aff" rid="B1">a</xref></contrib></contrib-group><aff id="B1"><label>a</label>Department of Internal Medicine, Yale School of Medicine, New Haven, Connecticut</aff><aff id="B2"><label>b</label>Yale Law School, New Haven, Connecticut</aff></front-stub><body><p>Despite tremendous and increasing clinical opportunities for cure and comfort, patients often still feel dissatisfied in their relationship with their doctor. That patient dissatisfaction has endured, even in the face of increasing medical knowledge and capacity, suggests a failing not in the quality of medical treatment but in the way it is administered. Increasingly, the modern medical movement toward patient-centered medical care, and away from doctor-centered care, has attempted to address this failing, looking to patient satisfaction as one of its primary measures of success in these efforts. However, its willingness to overlook the importance of the basis of reported satisfaction belies its deeper, if unconscious, aim: to allow doctor and patient to avoid confronting deep-seeded ambivalence that each feels toward the other, inherent in their relationship. The opposing urges constitutive of this ambivalence threaten to reverse physicians&#x02019; hard-won, positive self-concept, anchored in their sense of beneficence. Faced with this threat, physicians often flee to the seemingly safer psychological territory of strict adherence to professional norms. But far from finding safety in these norms, many physicians feel failed by them and their promise of protection from the harms of deep involvement with patient turmoil. Thus unprotected, physicians often breach these norms in effort to protect themselves. This loss of standing with their sense of professional commitment, however, leaves them feeling further betrayed, now by themselves. Caught between a loss of protection and a loss of standing, doctors often feel disaffected and deeply embattled, as do the patients who bear this outcome. Unable to sustain these complex feelings, doctors often engage the problems of patient care in ways that promise to conceal these feelings. The false premise of this engagement, however, undermines physician authenticity and disables patient-centered care. How, then, can the doctor be restored to the feeling of authenticity he/she needs to stay with his/her patients in the midst of the tremendous and tremendously evocative ambivalence posed by serious illness? If physicians are unaware of the negative counter-transference activated in such evocative circumstances, they will be unaware of the danger that the treatment plans they pursue aim at least as much at self-protection as at patient care. This is the loss of patient-centeredness wrought by physician inauthenticity. Thus, this thesis contends that the deeply ambivalent feelings that commonly trouble physicians, far from requiring suppression, ought to have a role in the care of the patients they are thought to threaten, if the doctors who have them are to be restored to themselves and so, too, to the patients depending on them.</p></body></sub-article><sub-article id="d31e1624"><front-stub><title-group><article-title>Continuity of Care for Pain, Depression, and Psychosis in Older Adults</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Sophia</given-names></name></contrib><contrib contrib-type="author"><name><surname>Boockvar</surname><given-names>Kenneth S.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Morrison</surname><given-names>Sean</given-names></name></contrib></contrib-group><aff>Department of Geriatrics and Adult Development, Mount Sinai School of Medicine, New York, New York</aff><author-notes><fn><p>Sponsored by Paul Kirwin, Department of Psychiatry, Yale University School of Medicine, New Haven, Connecticut</p></fn></author-notes></front-stub><body><p>This study examines factors influencing interruptions in chronic neuropsychiatric medications (opioids, antipsychotics, and antidepressants) during episodes of acute illness in nursing home patients. We also examine the influence of opioid interruption on delirium, pain, and withdrawal symptoms during an acute illness.</p><p>In this observational cohort study, 61 participants on chronic neuropsychiatric (NP) medications were enrolled from two nursing homes in the New York City metropolitan area from October 2006 to December 2007. Enrolled patients were assessed at baseline, monthly, and during acute illness for symptoms of pain, delirium, psychosis, depression, and opioid or antidepressant withdrawal. Medical records were abstracted at enrollment and after each episode of acute illness. NP medication administration was charted daily during the acute illness assessment period.</p><p>Of the 399 patients screened, 110 were eligible for enrollment and 61 consented for an enrollment rate of 55 percent. In the study population, 33.3 percent were receiving opioids, 28.3 percent antidepressants, 15 percent antipsychotics, and 18 percent were receiving more than one of these classes. There were 88 acute illnesses, including 26 hospital transfers. In multivariate regression, NP medication interruption was associated with opioid use (compared to antidepressant or antipsychotic use) with an OR 6.928 and 95 percent CI 1.385 &#x02013; 34.657. Interruption was inversely associated with increased patient agitation at the onset of acute illness, hospital admission, or hospital discharge (OR 0.027, 95 percent CI &#x0003c; 0.001 &#x02013; 0.978). For all acutely ill patients prescribed opioids, there was less delirium on opioid interrupted days than on continuation days (p = 0.0395). For the subset of these patients in whom opioids were interrupted, pain was significantly worse during vs. after (p = 0.006) and before vs. after the interruption (p = 0.004). There was no significant difference in withdrawal symptoms for opioid interruption and non-interruption periods.</p><p>Our results indicate that for nursing home patients on opioids who become acutely ill, the clinician encounters a trade-off in the benefits obtained from improved pain by continuing opioids and the increased risk of delirium.</p></body></sub-article><sub-article id="d31e1664"><front-stub><title-group><article-title>Differences in the Central Neural Activation under Emotional Stress across the Menstrual Cycle</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Lopushnyan</surname><given-names>Natalya</given-names></name></contrib><contrib contrib-type="author"><name><surname>Lacadie</surname><given-names>Cheryl</given-names></name></contrib><contrib contrib-type="author"><name><surname>Hong</surname><given-names>Adam</given-names></name></contrib><contrib contrib-type="author"><name><surname>Sinha</surname><given-names>Rajita</given-names></name></contrib></contrib-group><aff>Department of Psychiatry, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>The present study had several goals. First, we aimed to investigate the potential differences in the activation of the corticolimbic structures during emotional stress in healthy women across the menstrual cycle using stress imagery. Second, we searched for differences in the subjective anxiety under emotional stress across the menstrual cycle and tried to correlate the perceived level of anxiety to activation of the specific corticolimbic structures. Third, we attempted to compare central neural activation of women in follicular and in luteal phases of the menstrual cycle separately to that of men during emotional stress to investigate potential differences in neural response. We used perfusion-based functional magnetic resonance imaging (MRI) and blood oxygen level dependent (BOLD) contrast to measure cerebral blood flow response to the emotional stress using stress imagery in 29 healthy volunteers (nine women in follicular phase, 10 women in luteal phase, and 10 men). Cycle-dependent comparison of the stress response in women revealed that women in the follicular phase had greater activation in the areas of the ventro-medial prefrontal cortex (VMPFC), with levels of activation comparable to those of men, and anterior insula, while women in the luteal phase of their menstrual cycles demonstrated increase blood flow in the areas of the anterior cingulate and hippocampus at P = 0.01. Males showed an overall greater degree of corticolimbic activation, specifically in the bilateral hippocampi and right prefrontal cortex, when compared to either group of women. When compared to women in different phases of the menstrual cycle specifically, men showed greater cerebral blood flow in bilateral cingulate cortices and right hippocampus compared to women in the follicular phase, and in the bilateral striatum, amygdala, and bilateral hippocampi when compared to women in the luteal phase. We did not observe different levels of self-reported anxiety during stress imagery across the menstrual cycle; however, women in their luteal phase showed a positive correlation of the self-reported anxiety levels and cerebral blood flow in the posterior insula at the threshold level of P = 0.05. The results of our study are consistent with the previously available information regarding the differences in the corticolimbic activation across the menstrual cycle in women and in women vs. men. In addition, our data support the correlation of the levels of anxiety and insular activation in the luteal phase of the menstrual cycle and could represent an initial step in uncovering the mechanisms regulating stress response and anxiety and their relation to the hormonal status.</p></body></sub-article><sub-article id="d31e1699"><front-stub><title-group><article-title>The Use of Traditional Chinese Medicine and its Possible Relationship with Adherence to Highly Active Anti-Retroviral Therapy in Patients with HIV/AIDS in Hong Kong</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ma</surname><given-names>Kurtland</given-names></name></contrib><contrib contrib-type="author"><name><surname>Chu</surname><given-names>Elsie</given-names></name></contrib><contrib contrib-type="author"><name><surname>Khoshnood</surname><given-names>Kaveh</given-names></name></contrib><contrib contrib-type="author"><name><surname>Barry</surname><given-names>Michele</given-names></name></contrib><contrib contrib-type="author"><name><surname>Wong</surname><given-names>K.H.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Lee</surname><given-names>S.S.</given-names></name></contrib></contrib-group><aff>Department of Epidemiology and Public Health, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>The aim of this project was to examine the use of traditional Chinese medicine (TCM) and its possible relation to adherence to highly active anti-retroviral therapy (HAART) by HIV-infected Chinese male patients by answering the following questions:</p><p>                <list list-type="order"><list-item><p>How prevalent is TCM use among HIV-infected Chinese patients?</p></list-item><list-item><p>What types of TCM are used by this population?</p></list-item><list-item><p>What are the predictors of TCM use in this population?</p></list-item><list-item><p>Is there a relationship between TCM use and adherence to HAART? </p></list-item></list>            </p><p>Eighty-one self-administered questionnaires were completed by ethnic Chinese patients who had been on HAART for at least one year at Integrated Treatment Centre, a public HIV clinic run by the Hong Kong Department of Health.</p><p>Among study participants, 62 percent (n = 50) reported having taken any form of TCM at least once. Thirty forms of TCM were used by the 50 TCM-using patients, of which 26 percent (n = 13) were infrequent users, while 72 percent (n = 36) were regular users. The most frequent forms were: prescriptions provided by TCM practitioners (56 percent, n = 28), over-the-counter TCM preparations (52 percent, n = 26), and herbal teas (28 percent, n = 14). During the month preceding the study, 67 percent of participants (n = 54) had full HAART adherence and 33 percent of patients (n = 27) reported having missed one dose or more.</p><p>Results did not suggest any definite relationship between TCM use and HAART adherence. However, we did find a wide range of forms of TCM used by this patient population. Further, our findings suggest that TCM use among participants was not used to treat HIV/AIDS or related complications. Further study is needed to understand the biological activity of these TCM remedies and any possible interactions with HAART.</p></body></sub-article><sub-article id="d31e1768"><front-stub><title-group><article-title>Diminished Circulating Monocytes after Peripheral Bypass Surgery for Critical Limb Ischemia</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Magri</surname><given-names>Dania</given-names></name></contrib><contrib contrib-type="author"><name><surname>Dardik</surname><given-names>Alan</given-names></name></contrib></contrib-group><aff>Department of Surgery, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>Mononuclear cells (MNCs) have been shown to increase neovascularization and ulcer healing after direct injection into the ischemic limbs of patients with unreconstructable peripheral vascular disease (PVD). Circulating MNCs are composed of lymphocytes (85 percent), monocytes (15 percent), and endothelial progenitor cells (EPCs; 0.03 percent). It is thought that MNCs may be effective in ameliorating ischemia since EPCs are a component of the monocyte fraction, and EPCs have been shown to participate in vascular healing.</p><p>We hypothesized that ischemic areas secrete paracrine signals such as cytokines and growth factors that recruit bone marrow-derived monocytes into the circulation in order to augment vascular healing. For this reason, we predicted that patients with critical limb ischemia (CLI) undergoing bypass surgery would have elevated preoperative monocyte counts compared to control subjects without CLI. In addition, since a successful surgical bypass procedure relieves ischemia, we expected a postoperative decrease in circulating monocyte numbers.</p><p>We reviewed the records of all patients at the VA Connecticut Healthcare System undergoing lower extremity peripheral bypass surgery between 2002 and 2007. Patients were excluded if they did not have both preoperative and postoperative complete blood counts with differentials within a given time frame. Subjects were divided into two groups: those with preoperative critical limb ischemia (CLI) and those without. ANOVA and Chi-Square tests were used to compare counts, and multivariable logistic regression was used to determine risk factors.</p><p>Patients with CLI (n = 24) had elevated preoperative monocyte counts compared to control patients (n = 8) undergoing bypass for claudication or asymptomatic popliteal aneurysm (0.753 &#x000b1; 0.04 vs. 0.516 &#x000b1; 0.05; p = 0.0046), but the preoperative lymphocyte count was not significantly different (1.979 &#x000b1; 0.14 vs. 1.912 &#x000b1; 0.22; p = 0.814). After revascularization, ischemic patients had decreased monocyte counts compared to control patients (-20 percent vs. +55 percent; p = .0003) although lymphocyte ratios were unchanged in both groups (-10 percent vs. +1 percent; p = 0.404). Diabetic patients also had reduced postoperative monocyte counts (-32 percent vs. +13 percent; p = 0.035); however, multivariable analysis demonstrated that the only factor that independently predicted reduced postoperative monocyte count was preoperative critical limb ischemia (p = 0.038).</p><p>Diminished numbers of circulating monocytes correlate with relief of ischemia after surgical revascularization. Circulating monocytes may be a clinically useful surrogate marker of circulating stem cells for patients undergoing vascular surgery.</p></body></sub-article><sub-article id="d31e1799"><front-stub><title-group><article-title>Effect of Black Cohosh on MCF-7 Human Breast Cancer Cell Growth and Response to Treatment with Doxorubicin and Paclitaxel</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Marsh</surname><given-names>Julia</given-names></name></contrib><contrib contrib-type="author"><name><surname>Rockwell</surname><given-names>Sara</given-names></name></contrib></contrib-group><aff>Department of Therapeutic Radiology, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>Black cohosh (<italic>Cimicifuga racemosa</italic>), a shrub-like plant native to North America, frequently is used to mitigate the climacteric symptoms of menopause. The prevalent use of black cohosh by women with breast cancer, both during and after treatment, has raised concerns about the potential effects on breast cancer growth and interactions with anticancer therapies. Recent studies also have found that black cohosh actually may inhibit human breast cancer cell growth and sensitize cancer cells to commonly utilized chemotherapeutic agents. This study aims to investigate the effect of a commercially available black cohosh extract on the growth and viability of MCF-7 human breast cancer cells and on their response to the chemotherapeutic agents docorubicin and docetaxel. MCF-7 cells, a well characterized estrogen-sensitive human breast cancer cell line, were exposed to a commercially available black cohosh extract (GAIA herbs) standardized by the manufacturer to contain 3 percent triterpene glycosides at doses calculated to be 1x, 10x, and 100x the recommended human dose. Control cells were treated with the vehicle used to prepare the extract. The effect of black cohosh alone on cell proliferation and viability was determined both by cell growth assays, in which cell counts were obtained each day for one to five days after addition of the extract to MCF-7 cultures, as well as by colony formation assays for clonogenicity. To determine the possible effects of the black cohosh extract on the response of MCF-7 cells to chemotherapy, clonogenicity assays were performed to assay the survival of cells that were pretreated with either black cohosh or a vehicle control prior to exposure to doxorubicin or docetaxel. To study the effects on drug transport, efflux studies in MCF-7 cells treated with either black cohosh or vehicle prior to, during, and after exposure to doxorubicin were also performed. Black cohosh extract at 100x the human dose resulted in a significant decrease in the growth and viability of MCF-7 human breast cancer cells. Lower doses of the extract had no significant effects on cell growth. The highest dose of black cohosh extract also reduced the clonogenicity of the MCF-7 cells, suggesting a cytotoxic effect as well as an inhibition of cell proliferation. Interestingly, this study found no significant effect of black cohosh on the dose-response curves for MCF-7 cells exposed to graded doses of the chemotherapeutic agents doxorubicin and docetaxel. This study did not show inhibition of drug efflux by black cohosh. The results from this study are in contrast to those found previously in the estrogen-independent mouse mammary cancer cell line EMT6. The reason why the extract modulates the response to doxorubicin and docetaxel in EMT6 cells but not MCF-7 cells is currently under investigation. We hypothesize that it may reflect differences in efflux mechanisms in the two cell lines or effects on proliferation-dependent DNA repair pathways.</p></body></sub-article><sub-article id="d31e1825"><front-stub><title-group><article-title>Radiation is an Important Component of Multimodality Therapy for Pediatric Supratentorial Non-Pineal Neuroectodermal Tumors</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>McBride</surname><given-names>Sean M.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Daganzo</surname><given-names>Sally M.</given-names></name><xref ref-type="aff" rid="C1">a</xref></contrib><contrib contrib-type="author"><name><surname>Banerjee</surname><given-names>Anuradha</given-names></name><xref ref-type="aff" rid="C2">b</xref></contrib><contrib contrib-type="author"><name><surname>Gupta</surname><given-names>Nalin</given-names></name><xref ref-type="aff" rid="C3">c</xref></contrib><contrib contrib-type="author"><name><surname>Lamborn</surname><given-names>Kathleen R.</given-names></name><xref ref-type="aff" rid="C3">c</xref></contrib><contrib contrib-type="author"><name><surname>Prados</surname><given-names>Michael D.</given-names></name><xref ref-type="aff" rid="C3">c</xref></contrib><contrib contrib-type="author"><name><surname>Berger</surname><given-names>Mitchel S.</given-names></name><xref ref-type="aff" rid="C3">c</xref></contrib><contrib contrib-type="author"><name><surname>Wara</surname><given-names>William M.</given-names></name><xref ref-type="aff" rid="C1">a</xref></contrib><contrib contrib-type="author"><name><surname>Haas-Kogan</surname><given-names>Daphne A.</given-names></name><xref ref-type="aff" rid="C1">a</xref><xref ref-type="aff" rid="C3">c</xref></contrib></contrib-group><aff id="C1"><label>a</label>Department of Radiation Oncology, University of California, San Francisco, California</aff><aff id="C2"><label>b</label>Department of Pediatrics, University of California, San Francisco, California</aff><aff id="C3"><label>c</label>Department of Neurological Surgery and Brain Tumor Research Center, University of California, San Francisco, California</aff><author-notes><fn><p>Sponsored by Lynn Wilson, Department of Therapeutic Radiology, Yale University School of Medicine, New Haven, Connecticut</p></fn></author-notes></front-stub><body><p><bold>Purpose</bold>: We reviewed a historical cohort of pediatric patients with supratentorial primitive neuroectodermal tumors (sPNET) in order to clarify the role of radiation in the treatment of these tumors.</p><p><bold>Patients and Methods</bold>: Fifteen children younger than 18 with non-pineal sPNETs diagnosed between 1992 and 2006 were identified. Initial therapy consisted of surgical resection and chemotherapy (CT) in all patients and up-front radiotherapy (RT) in five patients. Five patients had RT at the time of progression, and five received no RT whatsoever. Kaplan-Meier estimates of overall survival (OS) were then calculated.</p><p><bold>Results</bold>: The median follow-up from diagnosis for all patients was 31 months (range 0.5-165) and for surviving patients, 49 months (range 10-165). Of the five patients who received up-front RT, all were alive without evidence of disease at a median follow-up of 50 months (range 25-165). Only five of the 10 patients who did not receive up-front RT were alive at last follow-up. There was a statistically significant difference in overall survival between the group of patients that received up-front RT and the group that did not (P = 0.048). Additionally, we found a trend toward a statistically significant improvement in overall survival for those patients who received gross total resections (P = 0.10).</p><p><bold>Conclusions</bold>: Up-front radiotherapy and gross total resection may confer a survival benefit in patients with sPNET. Local failure was the dominant pattern of recurrence. Efforts should be made to determine patients most likely to have local failure exclusively or as a first recurrence in order to delay or eliminate cranio-spinal irradiation (CSI).</p></body></sub-article><sub-article id="d31e1936"><front-stub><title-group><article-title>A Historical Analysis of Physician Dissatisfaction</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>McDonald</surname><given-names>Lisa E.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Warner</surname><given-names>John H.</given-names></name></contrib></contrib-group><aff>Section of the History of Medicine, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>Given the growing levels of physician dissatisfaction and attrition, this study was conducted to ask: What was the historical development of the events that gave rise to the problematic conditions, what are the characteristics of the current daily practice environment that create dissatisfaction, and what are the consequences of physician dissatisfaction? This research was conducted by literature review compiled from both the lay and scientific sectors; however, when possible, primary sources were employed. As a science, medicine undoubtedly has improved over the last century. However, this review would suggest that developments in the last 30 years have created a contemporary practice environment that fosters physician unrest by removing positive incentives for practicing medicine. Two of the most consistently cited factors were managed care and the malpractice crisis. Physician dissatisfaction has been demonstrated to result in a wide range of consequences, from the personal (burnout, attrition) and professional (inappropriate patient care) to the disastrous (suicide). The best next step recommended is to raise awareness and open dialogue early in the medical education process to prepare students for the realities of a life in medicine.</p></body></sub-article><sub-article id="d31e1959"><front-stub><title-group><article-title>Pediatric Consultations to Child Protective Services: The Role of Expert Opinions</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>McGuire</surname><given-names>Lindsay</given-names></name></contrib><contrib contrib-type="author"><name><surname>Martin</surname><given-names>Kimberly</given-names></name></contrib><contrib contrib-type="author"><name><surname>Leventhal</surname><given-names>John M.</given-names></name></contrib></contrib-group><aff>Department of Pediatrics, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>The purpose of this study was to provide a description of the types of child abuse consultations done by experts at Yale-New Haven Children&#x02019;s Hospital (YNHCH) to Child Protective Services (CPS) in Connecticut, and compare the opinions on the likelihood of child maltreatment from the separate perspectives of the initial treating physician, CPS, and the child abuse expert.</p><p>Eligible cases were referred by CPS for expert child abuse consultations at YNHCH between March 1998 and June 2005. Abstracted information included demographics, information about the type of injury, and the assessments of the case by the initial treating physician, CPS, and the child abuse expert.</p><p>Of 187 cases, 49 percent were males; 30 percent were African American, 28 percent white, 23 percent Hispanic, 1 percent Asian, and 17 percent were of unknown ethnicity. The types of cases, defined by the most serious overall injury to the child varied: 46 percent involved fractures; 22 percent bruises, scars, or abrasions; 13 percent burns; 4 percent brain hemorrhages; 2 percent death; 1 percent retinal hemorrhage; and 7 percent other injuries. In 5 percent of cases, no physical injury to the child was found by any of the assessors.</p><p>In 57 percent (N = 68) of the 119 cases that had opinions by all three assessors, the expert agreed with the opinion of the original physician on the case; of these 68 cases, 44 were abuse, six were uncertain, and 18 were accidental. In 65 percent (N = 77) of cases the expert agreed with CPS on the case; of these cases, 50 were abuse, two were uncertain, and 25 were accidental. The expert was more likely to determine the case to be accidental (49 percent of cases), compared to CPS (25 percent of cases) and the original physician (18 percent of cases). When CPS believed the case to be abuse (69 cases), the expert most often agreed it was abuse (72 percent), but in 22 percent of cases disagreed and determined the case to be accidental.</p><p>Overall, the expert thought 48 percent of the cases were abuse. When no explanation was given by the caretaker to account for the child&#x02019;s injury (N = 18), the expert was very likely to find the case one of abuse (72 percent); in contrast, when an explanation was provided by the caretaker for a fall (N = 45) or an accident by the child (N = 16), the expert was less likely to find the case one of abuse (35 percent and 31 percent, respectively).</p><p>Factors that increased the probability of the expert determining the case to be one of abuse included: the child had a medical or psychological problem, the child was on record with CPS for a previous concern of maltreatment, CPS removed the child from the home or provided parental services/education, and the child had three or more injuries.</p><p>Although there was agreement in a substantial proportion of cases, in a significant number of cases the child abuse expert had opinions that differed from CPS and the treating physician. Therefore, second opinions by child abuse experts have important value in selected child abuse cases, both to confirm previous assessments by the treating physician and CPS and to change the opinion of the case.</p></body></sub-article><sub-article id="d31e2000"><front-stub><title-group><article-title>Clinical and Radiographic Assessment of Lumbar Spine Total Disc Replacement in Athletes with Two-Year Follow-Up</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>McRae</surname><given-names>Mark H.</given-names></name></contrib><contrib contrib-type="author"><name><surname>McRae</surname><given-names>Matthew C.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Yue</surname><given-names>James J.</given-names></name></contrib></contrib-group><aff>Department of Orthopaedic Surgery, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>The purpose of the study was to evaluate the consequences of athletic activity on the clinical and radiographic outcomes of lumbar spine total disc replacement (TDR) patients.</p><p>The data for this study is drawn secondarily from a prospective randomized study evaluating the Prodisc prosthesis at Yale New Haven Hospital. Athletic activities prior to the onset of spinal injury, after the onset of spinal injury, and post lumbar spine total disc replacement (TDR) surgery were assessed. Athletic activity was classified into three groups. These were contact/vigorous, moderate, and light, based on effect on the involved spinal segments. Outcomes were assessed both clinically and radiographically.</p><p>Out of 195 patients enrolled in the lumbar spine Prodisc study at Yale, 82 qualified for inclusion and fulfilled all follow-up criteria. In these 82 patients, 120 disc replacements were performed. The average reduction from pre-operative visual analog pain scale was 44 (std dev 30.1) at a minimum of two years follow-up. The average reduction in Oswestry disability index was 38 percent (std dev 23). Seventy-four of 82 patients returned to athletic activity following TDR. Nineteen (23 percent) patients returned to pre-injury athletic activity levels, 47 (57 percent) returned to athletic activity but not to pre-injury levels, 14 (17 percent) patients reported activity levels that were unchanged since surgery, and two (3 percent) had activity levels become more impaired since surgery. Of those who returned to athletic activity, four of 74 patients complained of radiculopathy symptoms during athletic participation. Overall, 14 of 82 patients reported persistent back pain, and eight of these patients reported radiculopathy symptoms. Segmental flexion and extension at the levels of the implant, and the levels adjacent, revealed that the goal of physiologic motion was not reached at either the level of the implant nor at the superior or inferior adjacent segments. Three L5/S1 subluxations occurred in heavy weight lifters and were the only radiographic complications.</p><p>Athletic activities of varying degrees appear to be well tolerated following lumbar TDR surgery in single and multi-level cases. Contact-vigorous athletic activities do not appear to result in high levels of clinical or radiographic complications in the lumbar TDR patients except for heavy weight lifting activities in patients who have undergone L5/S1 Prodisc surgery in which we experienced three implant subluxations. Further biomechanical and clinical studies are necessary before general recommendations can be made.</p></body></sub-article><sub-article id="d31e2035"><front-stub><title-group><article-title>Stress, Serotonin Transporter Genotype, and Emotion Processing in Children</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Meadows</surname><given-names>Amy L.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Herrington</surname><given-names>John</given-names></name></contrib><contrib contrib-type="author"><name><surname>Kaufman</surname><given-names>Joan</given-names></name></contrib></contrib-group><aff>Child Study Center and Department of Psychiatry, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>Child maltreatment is a significant contributor to psychopathology, including depressive disorders and post-traumatic stress disorder (PTSD). Multiple studies have demonstrated important changes in emotion processing and regulation in children exposed to early life stress, but the underlying neural mechanism is unknown. There is significant variability in outcome in children exposed to early life stress that may be moderated by genetic polymorphisms in the serotonin transporter. This study was designed as a functional Magnetic Resonance Imaging (fMRI) project using dichotic listening to study emotion-processing pathways and interhemispheric transfer.</p><p>Eighteen children were recruited in a two-by-two factorial design with maltreatment and serotonin genotype as factors. Main effects of genotype were seen in increased activation in the amygdala and orbitofrontal cortex (OFC) to attended angry stimuli. No main effects of trauma (either as a categorical or continuous variable) were seen, and there was no interaction of trauma and genotype. Non-significant differences were found in the pattern of emotion processing between the maltreatment groups seen in the superior temporal gyrus. The results are consistent with a growing body of literature that has identified genotype as an important factor in neural pathways. The increased amygdala and OFC activity was seen in the controls as well as the traumatized children with the vulnerable genotype. An emerging question is how these differences lead to psychopathology in some instances and not others.</p></body></sub-article><sub-article id="d31e2066"><front-stub><title-group><article-title>Psychosocial Functioning in Childhood Cancer Survivors, Measured by Parent, Teacher, and Child Surveys</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Miller</surname><given-names>Tamara Porter</given-names></name></contrib><contrib contrib-type="author"><name><surname>Kadan-Lottick</surname><given-names>Nina</given-names></name></contrib></contrib-group><aff>Department of Pediatrics, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>The purpose of the current study is to understand emotional and neurocognitive functioning in childhood cancer survivors.</p><p>In this single-evaluation cross sectional study, 41 childhood cancer survivors (53.2 percent of those eligible) in the Health Education Research Outcomes in Survivors (HEROS) Clinic at Yale-New Haven Hospital between the ages of 6 and 18 and their parents and teachers completed the Behavior Assessment System for Children, Second Edition (BASC-II), Behavior Rating Inventory of Executive Functioning (BRIEF) and Pediatric Quality of Life Inventory (PedsQL). Patients, parents, and teachers&#x02019; scores were compared to those of normative populations for emotional, neurocognitive and quality of life measures. Frequencies of impairment and within-population differences in the study sample were calculated. Responses by patients were compared to those of parents and teachers to determine inter-rater reliability. The patterns of co-existing neurocognitive and emotional difficulties in the sample were described. Lastly, elements of emotional functioning, neurocognitive impairment, and patient characteristics predictive of impaired quality of life were identified with unadjusted and multivariate analyses.</p><p>Overall, 56.1 percent of patients were female, the mean age at diagnosis was 3.6 years, and the mean age at study completion was 12.8 years. In examining inter-rater reliability, for most areas of emotional functioning there was poor agreement (kappa &#x0003c; 0.40). Parents and teachers showed moderate agreement in reporting problems with attention (kappa = 0.57), memory (kappa = .61), and metacognition (kappa = 0.52). Parents and children showed greater inter-rater reliability when reporting quality of life than symptoms, with agreement in every realm of the PedsQL (kappa &#x0003e; 0.40). Co-morbidities between emotional and neurocognitive impairments for the most part did not occur together. However, impairments in somatization and withdrawal tended to co-exist with impairments in memory, shift, and metacognition (p &#x0003c; 0.05). In the multivariate analyses looking at social, emotional, and school functioning, only neurocognitive functioning was a consistent predictor of poor quality of life (OR = 12.94, p = 0.008; OR = 11.48, p = 0.044; OR = 33.5, p = 0.003, respectively). Poor emotional functioning was also predicted by female gender (OR = 15.58, p = 0.025).</p><p>Similar to previous studies, a significant proportion of childhood cancer survivors in our sample endorse difficulties with internalizing symptoms and executive functions, as well as lower physical, emotional, and social functioning, than normative populations. The current study shows inter-observer variability, especially among indexes of emotional symptoms, indicating a need for multiple reporters to determine areas of deficit and true levels of functioning. Problems with memory, shift, initiation, and coordination of problem-solving behaviors co-exist with symptoms of somatization and social withdrawal in our study. However, it is the neurocognitive rather than emotional symptoms shown to be key predictors of how child-age survivors perceive their quality of life after therapy. These results highlight neurocognitive impairments as a target for intervention during and after treatment for pediatric cancers.</p></body></sub-article><sub-article id="d31e2095"><front-stub><title-group><article-title>Measures of Cardiovascular Health in Obese Youth</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Neal</surname><given-names>Ashley E.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Burgert</surname><given-names>Tania S.</given-names></name></contrib></contrib-group><aff>Department of Pediatrics, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>Among numerous other comorbidities, evidence continues to emerge supporting a relationship between childhood overweight and traditional markers of cardiovascular risk in adults. The ultimate ramifications of this association in terms of cardiovascular morbidity and mortality have not yet been fully elucidated. This study seeks to evaluate markers of cardiovascular health in a cohort of otherwise healthy, obese adolescents.</p><p>Baseline data was collected from 24 obese adolescents with normal glucose tolerance enrolled in a clinical trial of metformin therapy. In addition to anthropometric measurements, fasting levels of serum lipids and C-reactive protein were obtained and an oral glucose tolerance test was conducted. Peripheral arterial tonometry (PAT) testing, a novel, noninvasive approach for assessing endothelial dysfunction, was performed, and heart rate recovery was measured following a self-paced step test. Potential relationships between indices of metabolic control and measures of cardiovascular risk, including endothelial dysfunction and poor heart rate recovery, were considered.</p><p>No significant relationship could be demonstrated between results of peripheral arterial tonometry testing and measures of metabolic control or between this index and any other marker of cardiovascular health. However, there was a general trend toward impaired endothelial function with increasing postprandial glycemia. Additionally, no significant relationships were found between heart rate recovery and indices of glucose tolerance, but there was a general trend toward reduced heart rate recovery with increasing insulin resistance. Further research may support a role for noninvasive modalities such as PAT testing and heart rate recovery in longitudinal, as opposed to baseline, assessment of cardiovascular health in obese adolescents.</p></body></sub-article><sub-article id="d31e2123"><front-stub><title-group><article-title>Rhinovirus-Associated Wheezing and Asthma in Young Children</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Piotrowska</surname><given-names>Zofia</given-names></name></contrib><contrib contrib-type="author"><name><surname>Kahn</surname><given-names>Jeffrey S.</given-names></name></contrib></contrib-group><aff>Section of Pediatric Infectious Diseases, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>Human rhinoviruses (HRV) are a familiar cause of the common cold and are thought to be associated with asthma exacerbations in both children and adults. Recently, HRV have been identified as a major cause of hospitalization in children younger than 5. The purpose of this study was to determine whether HRV are also a cause of either wheezing and/or hospitalization in children younger than 2.</p><p>We used a PCR assay to screen for HRV infection in children younger than 2 with either 1) symptoms of upper or lower respiratory tract disease without wheezing; 2) with wheezing; 3) or those who were asymptomatic. A group of children who had a respiratory specimen submitted to a diagnostic laboratory for whatever reason and who tested negative for four common viruses in the clinical lab were also screened. All specimens were collected between January 1 and December 31, 2004. Phylogenetic analyses were performed on a majority of HRV isolates.</p><p>Overall, there were 28 (17 percent) of 165 children with symptoms of respiratory traction infection without wheezing; 21 (26.3 percent) of 80 children had symptoms of respiratory tract infection with wheezing; three (3 percent) of 93 were asymptomatic. Forty-seven (23.3 percent) of 202 children with specimens submitted to the diagnostic laboratory tested positive for HRV. The difference between the rate of infection in the asymptomatic group and each of the three other groups was statistically significant (p &#x02264; 0.01). Among children with samples submitted to the diagnostic laboratory in which HRV was the only identified pathogen, 55 percent were hospitalized. This rate was similar to that observed for respiratory syncytial virus (52.7 percent) among children of a similar age group and time period (P = 0.85). Diverse groups of HRV were circulating during the one-year study period.</p><p>We conclude that HRV are notorious pathogens among young children younger than 2 and are responsible for a significant proportion of wheezing in this age group. Among a group of children with a respiratory specimen submitted to the diagnostic laboratory in which a rhinovirus was the only identified pathogen, a majority were hospitalized.</p></body></sub-article><sub-article id="d31e2152"><front-stub><title-group><article-title>Do Caregivers Accurately Assess the Decision-Making Ability of their Cognitively Impaired Relatives?</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Rainof</surname><given-names>Mila N.</given-names></name><xref ref-type="aff" rid="D1">a</xref></contrib><contrib contrib-type="author"><name><surname>Lai</surname><given-names>James M.</given-names></name><xref ref-type="aff" rid="D1">a</xref></contrib><contrib contrib-type="author"><name><surname>Karlawish</surname><given-names>Jason H.</given-names></name><xref ref-type="aff" rid="D2">b</xref></contrib><contrib contrib-type="author"><name><surname>Drickamer</surname><given-names>Margaret A.</given-names></name><xref ref-type="aff" rid="D1">a</xref></contrib></contrib-group><aff id="D1"><label>a</label>Yale University School of Medicine, New Haven, Connecticut</aff><aff id="D2"><label>b</label>University of Pennsylvania, Philadelphia, Pennsylvania</aff></front-stub><body><p>Although caregivers are often faced with determining whether their relatives have the decision-making ability to solve everyday functional problems, it is unknown whether caregivers&#x02019; immediate opinions reflect more formal clinical assessments. In this study, we examine whether caregivers&#x02019; assessments of decision-making ability are consistent with measurements of decision-making ability using the Assessing Capacity in Everyday Decision-making (ACED). This study was cross-sectional, performed in an outpatient specialty clinic. For 39 patients with mild to moderate cognitive impairment, we asked caregivers a series of questions about the patient&#x02019;s overall and specific decision-making ability. Responses were scored on a five-point Likert scale. We measured patient decision-making ability using the ACED instrument. In our analysis, we examined the correlates of caregiver perceptions and determined its relationship with measured decision-making ability using multivariate linear regression.</p><p>The Spearman correlation coefficients between all caregiver perceptions of patient decision-making ability and ACED scores were less than 0.24 (P &#x0003e; .13). Multivariate analyses, adjusting for the time the caregiver spent with the patient, showed a non-significant relationship between the caregiver perceptions of decision-making as compared to the ACED and the amount of time the caregiver and patient were together. Caregiver opinion may not be reflective of actual measures of decision-making ability. Broader use of objective instruments may be needed to help accurately identify impaired decision-making ability. Disagreement between caregivers&#x02019; Likert scoring and ACED scores was not predicted by the amount of time the caregiver spent with the patient or by patient or caregiver gender. Caregivers were more likely to underestimate patient decision-making in the categories of meal preparation, medication administration, and overall. They were equally likely to over- and underestimate patient decision-making in the category of finances. The caregiver&#x02019;s relationship to the patient likely affects the likelihood of underestimation of decision-making ability, although this study was not large enough to confirm these results.</p></body></sub-article><sub-article id="d31e2203"><front-stub><title-group><article-title>The Chemokine MCP-1 is an Essential Mediator in Tissue Engineered Blood Vessel Development</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Roh</surname><given-names>Jason D.</given-names></name></contrib></contrib-group><aff>Interdepartmental Program in Vascular Biology and Therapeutics, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>Biodegradable tubular scaffolds currently are being evaluated as vascular grafts in the surgical treatment of congenital heart disease. Seeding scaffolds with bone marrow-derived mononuclear cells (BMCs) prior to implantation has been shown to significantly improve outcomes, including the long-term patency and vascular development of these grafts. However, the role these cells play is poorly understood. An IVC interposition model in an immunodeficient mouse recipient was used to evaluate the developmental remodeling process of engineered blood vessels, constructed from polyester tubular scaffolds seeded with human or mouse BMCs. Seeding scaffolds with BMCs significantly improved patency rates and graft development. However, seeded BMCs did not directly contribute to the cellularity of the developing vessel and were not detectable by three weeks. Rather, BMCs were found to produce significant amounts of Monocyte chemotactic protein-1 (MCP-1) in response to the scaffold biomaterials, resulting in early recruitment of recipient mouse monocytes to the scaffold and subsequent improvement in vascular neotissue formation. Seeding with mouse BMCs derived from MCP-1 knock out mice resulted in significant increases in late stenosis and graft wall thickening compared to wild type BMCs, demonstrating a direct role of MCP-1 in engineered blood vessel development and function. Furthermore, proper vessel development with increased early monocyte recruitment could be restored without the use of seeded BMCs by locally delivering MCP-1 directly from the scaffold. These results indicate that engineered blood vessels mature through an MCP-1 dependent pathway and present an &#x0201c;off-the-shelf&#x0201d; approach to vascular tissue engineering using acellular, chemokine-delivery scaffold systems.</p></body></sub-article><sub-article id="d31e2220"><front-stub><title-group><article-title>Nonobstetric Laparoscopy vs. Laparotomy During Pregnancy: Maternal and Fetal Outcomes</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ruby</surname><given-names>Jeannine A.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Prescott</surname><given-names>Jason D.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Roberts</surname><given-names>Kurt E.</given-names></name></contrib></contrib-group><aff>Section of Gastrointestinal Surgery, Department of Surgery, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>The purpose of this study was to compare maternal and fetal outcomes between nonobstetric laparoscopy and laparotomy during pregnancy at Yale-New Haven Hospital. A retrospective chart review was conducted of all nonobstetric intraabdominal surgeries during pregnancy at Yale-New Haven Hospital between 1987 and 2007. Of 159 potential cases, 103 cases (57 laparoscopies, 46 laparotomies) fit the criteria for analysis. Data were collected for the maternal surgical admission, maternal delivery admission, and infant outcome for both groups, and then were analyzed using Statistical Analysis Software (SAS) Version 9.1. There was no difference in age or BMI between groups. Mean gestational age at time of surgery was higher among laparotomy patients (21.1 &#x000b1; 7.9 weeks vs. 16.4 &#x000b1; 7.3 weeks, <italic>p</italic> &#x0003c; 0.05). There was no difference in the operative time between laparotomy and laparoscopy (79.8 &#x000b1; 31.8 min vs. 86.1 &#x000b1; 46.1 min, (<italic>p</italic> = 0.43). The postoperative length of stay associated with laparotomy was double that associated with laparoscopy (4.5 &#x000b1; 2.6 days vs. 2.2 &#x000b1; 1.7 days, <italic>p</italic> &#x0003c; 0.05). The postoperative complication rate was 47.4 percent after laparotomy and 17.4 percent after laparoscopy (<italic>p</italic> &#x0003c; 0.05). There were no maternal deaths. Three fetal losses occurred but did not reach statistical significance. Mean gestational age at delivery, Apgar scores, and rate of low-birth-weight infants were comparable between groups. Our data demonstrate that nonobstetric laparoscopy during pregnancy maintains the advantages of minimally invasive surgery and has better maternal and fetal outcomes than nonobstetric laparotomy during pregnancy.</p></body></sub-article><sub-article id="d31e2261"><front-stub><title-group><article-title>Establishing Prevalence Rate of Major Malformations after In Utero Exposure: A Challenge for Pregnancy Registries</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Russcol</surname><given-names>Ephat H.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Holmes</surname><given-names>Lewis B.</given-names></name></contrib></contrib-group><aff>Department of Pediatrics, Massachusetts General Hospital, Boston, Massachusetts</aff></front-stub><body><p>Baseline rates of malformations are affected by many different factors, and it is often difficult to determine what the actual rate of malformations is. A meta-analysis of rates of major malformations in the general population was performed. Several search terms were used and strict criteria utilized to select well-investigated studies that discussed the baseline prevalence of malformations in the population. Twenty-nine studies comprising 6,828,658 births were found, and 12 countries were represented. The mean (&#x000b1; standard deviation) malformation rate was 2.4537 &#x000b1; 1.2982, with a standard error of 0.2370. The malformation rate ascertained at birth or shortly after was 1.8188 &#x000b1; 0.6517, with a standard error of 0.1881. The malformation rate at age 1 year or older was 2.7561 &#x000b1; 1.0121, with a standard error of 0.2530. The malformation rate at birth was found to be significantly lower than the malformation rate ascertained at 1 year or older (p &#x0003c; 0.05).</p><p>This study is the first to show the statistically significant importance of age of ascertainment when looking across multiple studies. Pregnancy registers studying the effect of a prenatal exposure often do not have the resources needed to recruit an internal control group and therefore must choose an external control group. Many studies of baseline rates of malformations do not exclude chromosomal or genetic anomalies and ascertain malformations at birth and also later in life rather than only at birth. This results in a relatively high baseline or comparison malformation rate. In contrast, many pregnancy registers appropriately exclude chromosomal and genetic anomalies, and, due to resource constraints, they ascertain the malformation rate at birth or shortly thereafter. When malformation rates following an exposure are compared to a baseline rate that is erroneously higher than it should be, the teratogenic effect of the investigated exposure may be missed. It is dangerous to judge a medication&#x02019;s exposure as irrelevant when, in fact, it may be associated with a slightly higher malformation rate. Ideally, in the future, pregnancy registers will have internal control groups; the North American AED (antiepileptic drug) Pregnancy Registry has already begun to do so.</p></body></sub-article><sub-article id="d31e2286"><front-stub><title-group><article-title>Pre-pregnancy Body Mass Index, Hypertensive Disorders of Pregnancy, and Long-Term Maternal Mortality</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Samuels-Kalow</surname><given-names>Margaret</given-names></name></contrib><contrib contrib-type="author"><name><surname>Funai</surname><given-names>Edmund F.</given-names></name></contrib></contrib-group><aff>Department of Obstetrics, Gynecology, and Reproductive Services, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>Recent data have shown increased maternal mortality rates and cardiovascular disease after hypertensive disorders of pregnancy (HDP), but the reasons for this increase remain unclear. This study investigates the effects of a pre-pregnancy cardiovascular risk factor, namely body mass index (BMI), on the relationship between HDP and post-pregnancy mortality. Data came from a 1975-1976 subset (n = 13,722) of a population-based cohort of women. Multiple logistic regression was used to examine the risk of HDP by BMI by calculating odds ratios (OR) and 95 percent confidence intervals (CI). Age-adjusted Cox proportional hazards models were used to examine survival rates by calculation of the hazard ratios (HR) and 95 percent CIs. Normal weight was defined as BMI 18.5-24.9 kg/m2, overweight as BMI 25-29.9 kg/m2 and obesity as BMI 30 kg/m2. Women who entered their pregnancy overweight or obese had increased HDP [OR 2.82 (95 percent CI: 2.40, 3.31) and OR 5.51 (4.51, 7.31)] and decreased survival [HR 1.42 (1.10, 1.83) and HR 2.43 (1.61, 3.68)] when compared to normal weight women. HDP were associated with increased mortality in women who survived &#x0003e; 15 years [HR 1.94 (1.42, 2.67)], and that association remained significant, although attenuated, after adjustment for BMI [HR 1.65 (1.19, 2.79)]. The risk of death after HDP was increased in women who entered their pregnancy being overweight [HR 1.86 (1.07, 3.20)] or obese [HR 2.90 (1.28, 6.58)] as compared to the normal weight women [HR 1.26 (0.74, 2.14)]. Elevated pre-pregnancy BMI is associated with increased risk of HDP, which are known to be associated with an increased risk of maternal mortality. The association between HDP and mortality is thus increased in women entering pregnancy with elevated BMI. While obesity is a known cardiovascular risk factor, it does not fully explain the association between HDP and later life maternal disease. These data suggest that women with HDP should be followed after their pregnancy and appropriate interventions initiated to increase their long-term survival.</p></body></sub-article><sub-article id="d31e2309"><front-stub><title-group><article-title>Stress Fractures in Female Military Recruits: Is Increased Body Fat Protective?</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Sandoval</surname><given-names>Amanda</given-names></name></contrib><contrib contrib-type="author"><name><surname>Scott</surname><given-names>Christine</given-names></name></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Yuanzhang</given-names></name></contrib><contrib contrib-type="author"><name><surname>Testa</surname><given-names>Marcia</given-names></name></contrib></contrib-group><aff>Section of Quantitative Methods, Department of Public Health, Harvard School of Public Health, Boston, Massachusetts; Section of Epidemiology, Department of Preventive Medicine, Walter Reed Army Institute of Research</aff><author-notes><fn><p>Sponsored by Dr. BenHur Mobo, Department of Medicine, Yale University School of Medicine, New Haven, Connecticut</p></fn></author-notes></front-stub><body><p>In 2004, the Accession Medical Standards Analysis and Research Activity of the U.S. Army proposed the Assessment of Recruit Motivation and Strength (ARMS) study to determine whether those passing an initial screening fitness test were less likely to be injured during basic training than those unable to pass the test. Since 2005, qualified Army recruits who passed the ARMS test, but did not meet the body fat standard, were given a body fat waiver. This study was undertaken in order to evaluate if female Army recruits who exceed the allowable percent body fat, but successfully pass the ARMS fitness test, have the same hazard of lower extremity stress fracture compared with similarly fit female recruits who are weight and body fat qualified. Thus, the health records of 1,716 female recruits enrolled in the ARMS study from May 2004 to December 2006 and meeting eligibility criteria were reviewed. Stress fractures were identified by ICD-9 codes. The Kaplan-Meier estimator was used to analyze stress fracture-free survival for different weight and percent body fat categories. Multivariate analysis and stratified analyses were completed using Cox Proportional Hazards Models.</p><p>Three hundred fifty-eight female recruits were diagnosed with lower extremity stress fractures (21 percent). The hazard of lower extremity stress fracture in female recruits receiving an ARMS waiver compared to weight and body fat qualified recruits was 1.54 (CI 1.25 &#x02013; 1.88, p &#x0003c; 0.001). Rapid fitness index and age also were significantly associated with hazard of lower extremity stress fracture. Race and smoking status were not found to be significantly associated. The conclusions reached were that female recruits who exceed the Army&#x02019;s allowable percent body fat have an increased hazard of lower extremity stress fracture as compared to weight and body fat qualified female recruits. However, for highly fit female recruits, there is no association between body fat percentage and hazard of lower extremity stress fracture.</p></body></sub-article><sub-article id="d31e2350"><front-stub><title-group><article-title>Magnetic Resonance Findings of Exaggerated Fluid in Facet Joints Predicts Instability</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Schinnerer</surname><given-names>Kimberly A.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Katz</surname><given-names>Lee D.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Grauer</surname><given-names>Jonathan N.</given-names></name></contrib></contrib-group><aff>Department of Orthopaedics and Rehabilitation, Yale University  School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>The purpose of this study was to determine the incidence of exaggerated fluid signal in lumbar facet joints on Magnetic Resonance Imaging (MRI) and evaluate the correlation of this finding with radiographic evidence of instability. One hundred thirty-four consecutive lumbar MRIs obtained by a single surgeon over a two-year period were selected for review. Studies were evaluated for exaggerated fluid (defined as greater than one millimeter) between the articular surfaces of the facets on axial views. Standing plain films of all patients were then evaluated to determine the incidence of spondylolisthesis for patients with and without exaggerated fluid in the facets on MRI.</p><p>Of 134 consecutive MRIs, 118 were available for review. Sixteen (13.6 percent) had exaggerated fluid in the facets on axial images. Only two of these 16 (12.5 percent) had spondylolisthesis appreciable on MRI at that level. In contrast, eight of the 16 (50.0 percent) had spondylolisthesis at the level of exaggerated fluid when the corresponding radiographs were reviewed. Thus, spondylolisthesis was suggested in six of 14 cases (42.9 percent) when the exaggerated fluid sign was present but spondylolisthesis was not evident on the supine MRI.</p><p>By comparison, in the population without exaggerated fluid, only one in 102 (0.9 percent) showed a slip on plain film that was not observed on MRI. This difference was statistically significant (P &#x0003c; 0.001). The sensitivity and specificity for this finding in detecting spondylolisthesis were 57 percent and 92 percent, respectively. The positive predictive value was 50 percent, and the negative predictive value was 94 percent when using the presence of fluid in the facets on MRI as an indicator of radiographic lumbar instability. The positive diagnostic likelihood ratio was 7.43 and the negative diagnostic likelihood ratio was 0.46. Given a patient with fluid in the facets, the post-test probability of having spondylolisthesis was 93.0 percent.</p><p>In conclusion, patients with exaggerated fluid in the facets on axial MRI had a far greater likelihood of having spondylolisthesis on standing plain films than those without (odds ratio = 16.0, 95 percent CI, 4.44-57.60), even if this was not appreciated on the supine sagittal MRI sequences.</p></body></sub-article><sub-article id="d31e2385"><front-stub><title-group><article-title>Influence of a Serotonin Transporter Promoter Polymorphism on Corticolimbic Abnormalities in Bipolar Disorder</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Shah</surname><given-names>Maulik P.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Blumberg</surname><given-names>Hilary P.</given-names></name></contrib></contrib-group><aff>Department of Psychiatry, Yale University  School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>Bipolar disorder (BD) is associated with abnormalities of the subgenual anterior cingulate cortex (sgACC) and the amygdala, both components of a corticolimbic neural system that subserves emotional regulation. The short <italic>s</italic> allele &#x02014; as opposed to the long <italic>l</italic> allele &#x02014; of a serotonin transporter promoter (5-HTTLPR) polymorphism is associated with more severe course features of BD and impaired functional connectivity between the sgACC and amygdala in healthy control (HC) individuals. This study tests the hypothesis that the <italic>s</italic> allele influences the dysfunction in the sgACC-amygdala neural system in BD. Twenty-six euthymic BD participants (17 <italic>s</italic> carriers, nine <italic>ll</italic>) and 43 HC participants (31 <italic>s</italic>, 12 <italic>ll</italic>) completed an event-related functional magnetic resonance imaging scan while processing fearful, happy, or neutral faces. During fear and happy face processing, sgACC activation was significantly lower (p &#x0003c; 0.05) in the BD vs. the HC group, and in HC and BD <italic>s</italic> carriers compared to HC and BD <italic>ll</italic> individuals, respectively. In the sgACC region where BD activation was less than HC, response to emotional faces was lowest in the BD <italic>s</italic> group, suggesting that the <italic>s</italic> allele may contribute to more severe sgACC dysfunction in a subset of individuals that represent a distinct, genetically derived subtype within the heterogeneous BD clinical phenotype. Thus, sgACC dysfunction may be an endophenotype of BD, and the <italic>s</italic> allele appears to influence this dysfunction in a subset of BD individuals. Future treatment may be optimized for this subset by targeting treatments to affect this system and by further study of treatment response among those who carry the <italic>s</italic> allele.</p></body></sub-article><sub-article id="d31e2449"><front-stub><title-group><article-title>The Relationship between Liver Fat Content and Unenhanced Computed Tomography</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Shoebotham</surname><given-names>Karen</given-names></name></contrib><contrib contrib-type="author"><name><surname>Do</surname><given-names>Kinh Gian</given-names></name></contrib></contrib-group><aff>Department of Radiology, New York University School of Medicine, New York, New York</aff><author-notes><fn><p>Sponsored by Jeffrey C. Weinreb, Department of Diagnostic Radiology, Yale University School of Medicine, New Haven, Connecticut</p></fn></author-notes></front-stub><body><p>The purpose of this study was to chart unenhanced CT liver and spleen attenuation data on patients with normal and fatty livers to demonstrate a relationship between liver fat content and attenuation on unenhanced CT using MR chemical shift imaging as a reference standard. Retrospective data was gathered on 116 patients from NYU Tisch Hospital who had undergone both an unenhanced CT and MR opposed-phase imaging of the liver within two months. MR fat fraction (MR FF) was calculated using signal decrease on in-phase and opposed-phase imaging, with a cutoff of 0.2 considered significant fatty infiltration. Three CT liver attenuation indices were obtained: liver attenuation, liver minus spleen attenuation, and liver to spleen ratio. For each index, linear regression was performed and 95 percent confidence intervals calculated. Receiver operating characteristic (ROC) analyses were performed to determine the accuracy and optimal cutoff values for the best balance between sensitivity and specificity, and the highest cutoff value that had 100 percent specificity was determined. ROC analysis showed good accuracy of all CT indices, with sensitivities and specificities between 91 and 93 percent for cutoff values of 50, 0, and 1.0 for CT liver attenuation, liver minus spleen, and liver to spleen ratio, respectively. Correlation of hepatic steatosis between unenhanced CT and MR opposed-phase imaging was excellent, but as MR imaging is not the gold standard for determining liver fat content, conclusions drawn from this study depend on the accuracy of MR imaging as determined in other studies.</p></body></sub-article><sub-article id="d31e2476"><front-stub><title-group><article-title>Has the Radiologist Shortage Been Resolved? Recent Findings Using an Improved Survey Based Measurement</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Soni</surname><given-names>Krishan</given-names></name></contrib><contrib contrib-type="author"><name><surname>Bhargavan</surname><given-names>Mythreyi</given-names></name></contrib><contrib contrib-type="author"><name><surname>Sunshine</surname><given-names>Jonathan H.</given-names></name></contrib></contrib-group><aff>American College of Radiology, Reston, Virginia</aff><author-notes><fn><p>Sponsored by Howard P. Forman, Department of Diagnostic Radiology, Yale University School of Medicine, New Haven, Connecticut</p></fn></author-notes></front-stub><body><p>We report on a recently developed and improved measure by which the shortages or surpluses of physicians can perhaps be measured: the extent to which physicians desire more or less work if their income changes by the same percentage as their workload. Data from the American College of Radiology&#x02019;s (ACR&#x02019;s) 2007 Survey of Radiologists are used. Physicians were contacted via telephone and email by an outside contractor to assure confidentiality. Responses were weighted to be representative of all post-training professionally active radiologists in the United States. The author analyzed workloads and the desired workload changes for (i) radiologists who wanted less work; (ii) those who wanted more work; and (iii) those who sought no change. Characteristics of physicians in each of these three groups were analyzed. Multivariable regression analysis was employed to identify the probable causal links between characteristics of radiologists and the practices they work in with their desire for a workload change. The net average workload change sought in 2007 was approximately a 3 percent increase. By comparison, in 2003, radiologists on average did not desire a statistically significant change in workload. Subgroup analysis for 2007 indicates that, on average, radiologists working in government practices sought 26 percent more work, while those in multi-specialty private practices sought 4 percent more work. Those in the Northeast desired an average of a 7 percent increase, while in the Midwest and West, less than 1 percent change was desired. The overall balance between the demand and the supply of radiologists has shifted toward a surplus between 2003 and 2007. Based on our measure, we judge there was a balance in 2003 and a 3 percent surplus in 2007. There were differences in the surplus/shortage situation by type and location of practice.</p></body></sub-article><sub-article id="d31e2510"><front-stub><title-group><article-title>Evaluation of a Social Support Measure That May Indicate Risk of Depression During Pregnancy</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Spoozak</surname><given-names>Lori</given-names></name></contrib><contrib contrib-type="author"><name><surname>Gotman</surname><given-names>Nathan</given-names></name></contrib><contrib contrib-type="author"><name><surname>Smith</surname><given-names>Megan V.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Belanger</surname><given-names>Kathleen</given-names></name></contrib><contrib contrib-type="author"><name><surname>Yonkers</surname><given-names>Kimberly A.</given-names></name></contrib></contrib-group><aff>Department of Psychiatry, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>The objective of this study was to evaluate the psychometric properties of the Kendler Social Support Interview modified for administration to pregnant women and to assess the relationship between social support and depression in the first trimester of pregnancy. Subjects were administered the Modified Kendler Social Support Interview (MKSSI) and the Composite International Diagnostic Index to diagnose depression. Principal components analysis was employed to construct the MKSSI score. Cronbach&#x02019;s coefficient alpha and principal factor analyses were run for items included in the MKSSI score. The relationships among a depressive diagnosis, the MKSSI score, and subscales were assessed by logistic regression. Cronbach&#x02019;s coefficient alpha was high at 0.86. A one-unit increase in the MKSSI score (the difference between the 25th and 75th percentile) was associated with 58.3 percent lower odds of depression (OR = 0.417, 95 percent CI = 0.284-0.612). A higher MKSSI score, indicating greater social support, was significantly associated (p &#x0003c; 0.001) with reduced odds for depression in the first trimester. Subscales were factored by the source of support. A high subscale score for all relationships, except sibling relationships, was significantly associated (p &#x0003c; 0.05) with reduced odds for depression, but not as robustly as the total score. Therefore, the MKSSI is reliable and valid for use in pregnant women to assess social support.</p></body></sub-article><sub-article id="d31e2551"><front-stub><title-group><article-title>Palmitic Acid Increases Enac Expression and Contributes to Insulin Resistance in Renal Distal Tubule</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Sullivan</surname><given-names>Timmy B.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Canessa</surname><given-names>Cilia M.</given-names></name></contrib></contrib-group><aff>Department of Cellular and Molecular Physiology, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>Insulin resistance and obesity are two conditions associated with hypertension. Although the causes of hypertension are likely to be multifactorial, deregulation of renal distal tubule sodium reabsorption may play a role. It has been shown that insulin increases sodium re-absorption in the distal tubule through the increased expression of epithelial sodium channels (ENaC), while obesity has been shown to cause insulin resistance in various tissues. Thiazolidinediones (TZDs) are insulin-sensitizing drugs whose use can be limited by a side effect of fluid retention leading to edema and congestive heart failure. We set out to determine if the expression of ENaC in distal tubule cells, as modeled by the A6 (frog nephron) cells, became resistant to insulin following exposure to various concentrations of free-fatty acids (FFAs). We then set out to determine if TZDs had an insulin-sensitizing effect on distal tubule cells in both a FFA na&#x000ef;ve environment and after being exposed to FFAs. We exposed monolayers of A6 nephron cells to palmitic acid (PA) in concentrations of 0, 0.25, 0.5, 1, and 1.5 mM. After an incubation time of 20 hours, we stimulated the cells with insulin. Measuring the cells with equivalent short-circuit current, we found that as the PA concentration increased from 0 to 0.5 mM, there was an increase in current (Isc), implying an increase in ENaC activity, followed by a marked decrease in Isc at PA concentrations of 1 mM or greater. Consistent with an increase in Isc, the calculated transepithelial resistance decreased with 0.25 and 0.5 mM PA. However, at higher concentrations of PA, we observed a marked drop in resistance implying probable apoptosis. After insulin treatment, we found that in control cells (0 mM PA) the Isc increased 100 percent, whereas with increasing concentrations of PA there was a smaller fractional increase of 46 percent, 12 percent, and 0 percent at 0.25, 0.5, and 1.0 mM PA, respectively. After verifying that PPAR&#x003b1;, &#x003b2;, and &#x003b3; were expressed in these cells, we treated A6 cells with 10 &#x000b5;M of Rosiglitazone (PPAR gamma agonist) or 3 mM of Clofibrate (PPAR&#x003b1; agonist). We found there was no change in Isc following exposure to these drugs. We next pretreated the cells with both 0.5 mM PA with clofibrate or rosiglitazone. Again, there was no statistically significant difference between these conditions. Overall, we found that exposure to PA caused resistance to the insulin induced ENaC expression in A6 cells. At small concentrations of PA, there was increased base-line ENaC expression, but higher concentrations were toxic to the cells. We also found that TZDs did not have any effect on ENaC expression at base line, with insulin or exposure to PA.</p></body></sub-article><sub-article id="d31e2574"><front-stub><title-group><article-title>Monitoring, Identification, and Intervention for Metabolic Disorders in Veterans with Psychotic Disorders</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Swetye</surname><given-names>Michael H.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Ruser</surname><given-names>Christopher B.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Ranganathan</surname><given-names>Mohini</given-names></name></contrib><contrib contrib-type="author"><name><surname>Rohrbaugh</surname><given-names>Robert M.</given-names></name></contrib></contrib-group><aff>Department of Psychiatry, Veterans Affairs Connecticut Health Care System, West Haven, Connecticut</aff></front-stub><body><p>In light of growing evidence that certain antipsychotics may cause potentially life-threatening metabolic side effects, the purpose of this study was to determine how regularly mental health clinicians (MHCs) currently monitor and manage metabolic abnormalities in overweight and obese patients with psychotic disorders. We hypothesized that MHCs monitor, identify, and intervene for metabolic abnormalities in their patients at significantly lower rates than primary care physicians (PCCs) and that such rates may jeopardize patient health.</p><p>We performed a one-year cross-sectional medical record review of primary care and mental health routine outpatient visit notes from the West Haven campus of the Veteran Affairs Connecticut Healthcare System. We reviewed the records of a cohort of 123 veterans who met the following inclusion criteria: (1) primary diagnosis of schizophrenia or schizoaffective disorder; (2) at least one routine mental health visit at the West Haven VA facility between July 1, 2005, and June 30, 2006; and (3) overweight or obese, as determined by a body mass index (BMI) &#x02265; 25. We excluded all deaths.</p><p>The 123 subjects were predominantly white and male (56 percent and 93 percent, respectively) with an average body mass index (BMI) of 32.4 (SD = 5.4). 97 percent of subjects were taking an antipsychotic of some sort, and 85 percent were taking a second-generation antipsychotic.</p><p>Zero diagnoses of metabolic syndrome and zero waist-size measurements were documented by PCCs or MHCs. The following differences in documentation were found between PCCs and MHCs, respectively: weight (85 percent vs. 11 percent; p &#x0003c; 0.001); BMI (48 percent vs. 0 percent; p &#x0003c; 0.001); identified weight as an issue (45 percent vs. 28 percent; p &#x0003c; 0.005); identified the link between antipsychotics and weight issues (10 percent vs. 12 percent; not significant); made diet and exercise recommendations (42 percent vs. 19 percent; p &#x0003c; 0.001); ordered a weight-management referral (21 percent vs. 3 percent; p &#x0003c; 0.001); ordered or considered ordering a change of antipsychotic medication or dose due to weight-related issues (6 percent vs. 3 percent; not significant). PCCs ordered laboratory tests at much higher rates than MHCs, including blood glucose, thyroid stimulating hormone, urinalysis, lipid panel, and hemoglobin A1C (differences were large and significant).</p><p>We concluded that MHCs monitor, identify, and intervene for metabolic abnormalities in their patients at significantly lower rates than PCCs, and such rates are unacceptably low. The problem is one of a systemic failure in quality control and may pose a danger to patients. We advocate a rapid organizational response and systemic changes at the local and national level to improve quality.</p></body></sub-article><sub-article id="d31e2617"><front-stub><title-group><article-title>Role of MKL1 in Megakaryocytopoiesis and in t(1;22)-Associated Acute Megakaryoblastic Leukemia</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Troy</surname><given-names>James A.</given-names></name></contrib></contrib-group><author-notes><fn><p>Sponsored by Diane S. Krause, Department of Laboratory Medicine, Yale University School of Medicine, New Haven, Connecticut</p></fn></author-notes></front-stub><body><p>The t(1;22)(p13;q13) translocation creates a fusion of the genes RBM15 and MKL1 and is exclusively associated with the infantile form of acute megakaryoblastic leukemia, or AML variant M7. Although MKL1 is a known co-activator of the Serum Response Factor (SRF) pathway, it is not known whether MKL1 plays an important role in megakaryocytic commitment or differentiation. This project investigated the role of MKL1 in normal megakaryocytopoiesis by using a human CD34+ hematopoietic stem cell (HSC) model. Human CD34+ HSCs were transduced with lentiviral vectors expressing either MKL1 or dominant negative MKL1 (DN-MKL1) and were cultured in conditions promoting megakaryocytopoiesis. After nine days, cells transduced with MKL1-expressing virus had increased expression of the megakaryocyte-associated markers CD41a (52 &#x000b1; 10 percent vs. 28 &#x000b1; 15 percent), CD61 (56 &#x000b1; 15 percent vs. 26 &#x000b1; 13 percent), and CD42b (46 &#x000b1; 17 percent vs. 14 &#x000b1; 5 percent) when compared to cells transduced with the vector alone (all p &#x0003c; 0.05). Transduction with the DN-MKL1 lentivirus was also unexpectedly associated with an increase in these same markers, although this experiment was only performed once and will require further investigation. In separate megakaryocyte progenitor assays, MKL1 was associated with an increased frequency of megakaryocyte colony forming units (CFU-Mk) (263 &#x000b1; 111 vs. 164 &#x000b1; 61 CFU-Mk per 10,000 cells) although these data did not reach statistical significance. These findings suggest that MKL1 may be an important regulator of megakaryocytopoiesis and that abnormal MKL1 function may be involved in megakaryoblastic leukemogenesis.</p></body></sub-article><sub-article id="d31e2636"><front-stub><title-group><article-title>Use of Media to Improve Human Papilloma Virus (HPV) Vaccine Acceptability</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Venkat</surname><given-names>Pavithra</given-names></name></contrib><contrib contrib-type="author"><name><surname>Chapman</surname><given-names>Ella</given-names></name></contrib><contrib contrib-type="author"><name><surname>Ko</surname><given-names>Emily</given-names></name></contrib><contrib contrib-type="author"><name><surname>Garner</surname><given-names>Elizabeth</given-names></name></contrib></contrib-group><aff>Department of Obstetrics and Gynecology, Brigham and Women's Hospital, Boston, Massachusetts</aff><author-notes><fn><p>Sponsored by Jessica Illuzzi, Department of Obstetrics and Gynecology, Yale University School of Medicine</p></fn></author-notes></front-stub><body><p>Our objective was to determine if using a video educational tool can influence (1) individual vaccine acceptability, (2) parental acceptability, (3) parental views on vaccine mandates, and (4) age of vaccination accepted for the Human Papilloma Virus (HPV) vaccine. We conducted a cross-sectional study using bilingual surveys distributed at Brigham and Women&#x02019;s and Massachusetts General Hospital clinics and at the Coalition of Boston Public Health Association from January to March 2007. An initial 32-question survey addressing HPV knowledge, beliefs, and vaccine acceptability was completed, followed by an eight-minute video about HPV and the vaccine. An additional 11-question post assessment was then completed. Five questions were extracted from both the pre/post questionnaires to evaluate HPV vaccine acceptability. Out of 256 subjects, 186 (73 percent) completed the video intervention and pre/post surveys. Of the 186, 66.6 percent (124) of subjects said they would vaccinate themselves. Individual acceptability increased after the video to 78 percent (p = .0014). An additional 55.8 percent (102/186) of subjects supported making the HPV vaccine required for all children, with 51.1 percent (95/186) supporting vaccination if it were given at school and 66.7 percent (124/186) supporting child vaccination if it were free. After the video, this increased to 72.6 percent (p &#x0003c; .0001), 65.1 percent (p &#x0003c; .0001), and 86.6 percent (p &#x0003c; .0001), respectively. Initially, 56.5 percent (105/186) of subjects would vaccinate their child only if the child were older than 15; post-intervention, 82.3 percent of subjects accepted vaccination starting at age 9 and older (p &#x0003c; .0001). Secondary analysis revealed that Hispanic, blacks, and those with a combined income of less than $50,000 were more likely to not initially accept HPV vaccine for their children but showed high rates of acceptability after intervention. Perception that vaccination would promote sex among the young was significant but did not affect overall acceptability. In conclusion, using multi-media as a way to increase knowledge significantly increased individual acceptability, parental acceptability, and age of acceptance of the HPV vaccine.</p></body></sub-article><sub-article id="d31e2675"><front-stub><title-group><article-title>Older Adults&#x02019; Understanding of Cardiovascular Risk and Preventive Medication Benefit</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Wahl</surname><given-names>Elizabeth R.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Fried</surname><given-names>Terri R.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Agostini</surname><given-names>Joseph V.</given-names></name></contrib></contrib-group><aff>Section of Geriatrics, Department of Internal Medicine, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>While patients&#x02019; ability to make informed decisions about treatment priorities depends on their ability to understand the risks and benefits of medications and the likelihood of disease outcomes, patients&#x02019; knowledge of medication-related benefits is unexplored. We examined older adults&#x02019; estimation of their 10-year risk of myocardial infarction (MI) and stroke, and the degree to which they thought common medications were able to prevent these outcomes.</p><p>One hundred fifty male veterans age 65 or older taking five or more medications (including an aspirin, a statin, or an antihypertensive drug) were interviewed at VA Connecticut. Using a graph with bars representing 0, 10, 25, 50, 75, and 100 percent, participants were asked to estimate their 10-year risk of stroke and MI when (a) taking no medications and (b) when taking preventive medications (aspirin to prevent MI and stroke; statins to prevent MI; and anti-hypertensives to prevent stroke). Participants had a mean age of 76 &#x000b1; 6 years and were on 10 &#x000b1; 3 medications: 90 percent had hypertension, 76 percent had diabetes, 15 percent had prior MI, and 12 percent had prior stroke.</p><p>Framingham data suggest the 10-year risk of MI in this population is close to 25 percent, which decreases to about 15 percent on aspirin or statins. One hundred thirty of 147 (87 percent) participants overestimated their risk of MI (48 percent estimated it at 75 or 100 percent over 10 years), 37 (24 percent) participants felt that aspirin provided at least a 50 percent absolute risk reduction in MI, and 33 percent of participants felt that statins could reduce MI risk by the same degree. Notably, 18 percent of participants felt that daily aspirin did not change their MI risk at all, and 20 percent felt a daily statin did not change their MI risk.</p><p>For stroke, Framingham data suggest that 10-year risk in this population is close to 25 percent, which decreases to 15 percent on aspirin and anti-hypertensives. One hundred twenty-eight of 149 (86 percent) participants overestimated their stroke risk, with 90 (60 percent) estimating that risk to be 75 or 100 percent. Forty-six of 147 (31 percent) participants estimated that aspirin could provide a 50 percent absolute risk reduction in 10-year stroke risk, and 39 percent estimated that anti-hypertensives could provide at least a 50 percent absolute risk reduction. Eighteen (12 percent) participants felt that taking a daily aspirin did not change their 10-year stroke risk, while 18 percent felt that taking a daily anti-hypertensive did not change their 10-year stroke risk.</p><p>In summary, a large proportion of older males overestimated both their 10-year risk of MI and stroke. A portion of participants overestimated the magnitude of benefit conferred by aspirin, statins, and anti-hypertensive drugs in preventing adverse clinical outcomes while other participants underestimated it. Both findings have important implications for medication decision making, since underestimation of benefits may play a role in non-adherence, while overestimation of benefits may result in tolerance of medication side effects with the expectation that they provide a greater degree of benefit. This study suggests the need for increased patient-physician communication regarding the risks and benefits of commonly prescribed preventive medications.</p></body></sub-article><sub-article id="d31e2712"><front-stub><title-group><article-title>Community Based Epidemiological Study of Chagas Disease in Rural Peru</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Walker</surname><given-names>Paul C.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Gilman</surname><given-names>Robert H.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Levy</surname><given-names>Michael Z.</given-names></name></contrib></contrib-group><aff>Department of International Health, Johns Hopkins Bloomberg School of Public Health, Baltimore, Maryland</aff><author-notes><fn><p>Sponsored by Michele Barry, Department of Medicine, Yale University School of Medicine, New Haven, Connecticut</p></fn></author-notes></front-stub><body><p>The purpose of this study was to evaluate the epidemiology of <italic>Trypanosoma cruzi</italic> infection, to identify risk factors for seropositivity, to evaluate the TESA blot, and to characterize cardiac findings of patients with Chagas disease in a rural setting near Arequipa, Peru. The study site was the town of Queque&#x000f1;a, Peru, with a population of 774, with 236 inhabitants younger than 18, according to the 2005 census conducted by Peru. It is a rural, agricultural town in the southern highlands about 25 km south of Arequipa, the second largest city in Peru. The area was known to contain <italic>Triatoma infestans</italic>, the insect vector for <italic>T. cruzi</italic>, and a fumigation/insect collection campaign was conducted in December 2006 to quantify household infestation levels, document housing characteristics, and GPS household locations. Of the 602 residents of Queque&#x000f1;a surveyed, blood samples were taken from 445 (73.9 percent), and 15 (3.37 percent) were positive for Chagas disease by ELISA and confirmed by immunofluorescence. The TESA blot also was performed on all positives (N = 15) and a random subset of negative (N = 20) blood samples with a sensitivity of 93 percent and a specificity of 100 percent. Electrocardiograms (EKGs) were performed on 37 people, nine of whom were positive for Chagas disease, and the other 28 were age- and sex-matched controls. All EKGs of Chagas positive patients were normal, and 27 of 28 EKGs were normal in the control group. Of the 284 households in Queque&#x000f1;a, 242 (85.2 percent) were sprayed, and 58 (24.0 percent) were infested with triatomines. Nineteen households (7.85 percent) harbored triatomines infected with <italic>T. cruzi</italic>. Of the 15 patients positive for Chagas disease, nine lived in a house positive for triatomines (60 percent), of which three were positive for <italic>T. cruzi</italic> (33 percent).</p></body></sub-article><sub-article id="d31e2760"><front-stub><title-group><article-title>Intussusception of the Appendix: New Trends and Comprehensive Analysis of 140 Published Case Reports</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Wexelman</surname><given-names>Barbara A.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Chaar</surname><given-names>Cassius Ochoa</given-names></name></contrib><contrib contrib-type="author"><name><surname>Longo</surname><given-names>Walter</given-names></name></contrib></contrib-group><aff>Section of Colorectal Surgery, Department of Surgery, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p><bold>Statement of Purpose</bold>: This paper uses 139 published case reports to understand the demographic, diagnostic, and treatment trends of intussusception of the appendix.</p><p><bold>Methods</bold>: Using the PubMed literature search engine to find all English references of &#x0201c;intussusception&#x0201d; and &#x0201c;appendix,&#x0201d; and reviewing those that contained actual case reports of intussusception of the appendix, we analyzed the demographics, presentation, diagnostic methods, surgical treatment, and histology from 140 articles representing data from 181 patients.</p><p><bold>Results</bold>: There were 41 (22.5 percent) pediatric cases and 141 (77.5 percent) adult cases. The average age was 37.3 years. There were more males in the pediatric set (23 males vs. 18 females) while there were more females in the adult set (38 males vs. 101 females). The most prevalent symptoms in children were abdominal pain (87.8 percent), vomiting (53.7 percent), and nausea (26.8 percent). The adults presented with abdominal pain (75.4 percent), bloody stools (26.1 percent), and vomiting (18.1 percent). Most of the patients reported chronic symptoms (62.6 percent chronic, 30.8 percent acute). Barium enema was the most prevalent method for both pediatrics (43.9 percent) and adults (49.3 percent). The most common surgical procedure for both the children and the adults was appendectomy (43.9 percent), followed by right hemicolectomy (20.6 percent). Prior to 1990, the majority of IA cases were diagnosed intra-operatively (64.8 percent), but since 2000, over half of the patients (56.8 percent) were given the correct diagnosis pre-operatively, and less than one-third (29.6 percent) of patients were diagnosed intra-operatively. Endometriosis was the most common histopathology in adult women (37.6 percent).</p><p><bold>Conclusions</bold>: Adults, especially middle-aged women, make up the majority of patients with intussusception of the appendix. IA should be considered in the workup of chronic abdominal pain in women and may likely be linked with gastrointestinal endometriosis. Increasingly, IA is a pre-operative diagnosis, aided by colonoscopy and CT imaging.</p></body></sub-article><sub-article id="d31e2803"><front-stub><title-group><article-title>Electrical Stimulation and Glutamate in the Hippocampus of Epilepsy Patients</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Widi</surname><given-names>Gabriel A.</given-names></name></contrib></contrib-group><aff>Departments of Neurosurgery and Psychiatry, Yale University School of Medicine, New Haven, Connecticut</aff><author-notes><fn><p>Sponsored by Idil Cavus</p></fn></author-notes></front-stub><body><p>Electrical brain stimulation has been proposed as a promising treatment option for patients with medically resistant epilepsy disorder. Glutamate levels in the epileptogenic human hippocampus are elevated interictally and increase with seizures. Fifty Hz stimulation is a candidate therapeutic stimulation that is also used for clinical cortical mapping. We examined the effects of 50 Hz stimulation on glutamate efflux in the hippocampus of patients with medically refractory epilepsy. Subjects (<italic>n</italic> = 10) underwent intracranial EEG (icEEG) evaluation for possible therapeutic resection. Electrical stimulation was delivered through implanted hippocampal electrodes (<italic>n</italic> = 11) and microdialysate samples were collected every two minutes. Basal interictal glutamate was measured with the zero-flow microdialysis method. Stimulation of the epileptogenic hippocampus induced significant glutamate efflux at the time of stimulation (<italic>p</italic> = 0.005, <italic>n</italic> = 10) that was significantly related to the basal glutamate concentration (R&#x000b2; = 0.81, <italic>p</italic> = 0.001). During stimulation, four patients experienced seizures and two had auras. No change in glutamate level was observed in the group of patients who experienced a seizure (<italic>p</italic> = 0.47, <italic>n</italic> = 4). Conversely, a significant increase in glutamate was observed in the patients who did not experience a seizure (<italic>p</italic> = 0.005, <italic>n</italic> = 7). Basal glutamate levels were significantly higher in the no-seizure group (<italic>p</italic> = 0.04, <italic>n</italic> = 5) than in the stimulated seizure group (<italic>n</italic> = 4). Fifty Hz stimulation of the epileptogenic hippocampus can cause significant glutamate efflux and may produce seizures or auras. The degree of stimulated glutamate elevation is related to the basal glutamate concentration but not to the induction of seizures. Electrical stimulation at 50 Hz may exacerbate interictal glutamate dysregulation in the epileptiogenic hippocampus and may not be optimal for seizure control.</p></body></sub-article><sub-article id="d31e2862"><front-stub><title-group><article-title>The Changing Picture of Medical Diplomacy As Seen Through Scholarly Reports</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Wilkinson</surname><given-names>Indy McFall</given-names></name></contrib><contrib contrib-type="author"><name><surname>Forman</surname><given-names>Howard P.</given-names></name></contrib></contrib-group><aff>Department of Radiology, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>This thesis addresses the question: Did the geopolitical event of the terrorist attacks of September 11, 2001, cause a shift in the way the medical community views healthcare diplomacy? First, examples of past, present, and proposed medical and healthcare diplomacy programs are analyzed with respect to the intention, government affiliation, and diplomatic effect of each. Then, a group of 200 healthcare diplomacy articles from medical publications, half from five years prior to September 11, 2001, and half from five years following, were analyzed for word occurrence, nationality of publication, nationality of authorship, and governmental affiliation of authorship. A thorough analysis of the healthcare diplomacy articles reveals a dramatic increase in the use of words relating to terrorism, safety, and religion. In addition, there is a paradoxical decrease in healthcare policy articles authored by American physicians and epidemiologists. The observed trend shown in these scholarly reports provides evidence to support the theory that the terrorist attacks of September 11 caused a shift in the way the medical community views healthcare diplomacy.</p></body></sub-article><sub-article id="d31e2885"><front-stub><title-group><article-title>Efficacy of Formal Screening for Depression in Children and Adolescents with Type 1 Diabetes</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Winer</surname><given-names>Jeffrey Craig</given-names></name></contrib><contrib contrib-type="author"><name><surname>Hale</surname><given-names>Natalie</given-names></name></contrib><contrib contrib-type="author"><name><surname>Lavietes</surname><given-names>Sylvia</given-names></name></contrib><contrib contrib-type="author"><name><surname>Tamborlane</surname><given-names>William J.</given-names></name></contrib></contrib-group><aff>Section Of Endocrinology, Department Of Pediatrics, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>This study was undertaken to examine the efficacy of screening for depression in a pediatric diabetes clinic and how the yield of such screening compares with established screening protocols aimed at identifying early microvascular complications and associated autoimmune diseases. Two hundred fifteen children and adolescents ages 8 to 18 in the Yale Pediatric Diabetes Center were screened for depression using the Children&#x02019;s Depression Inventory (CDI), with information including gender, age, duration of diabetes, glycosylated hemoglobin levels (HbA1C), and results of other screening protocols compiled.</p><p>A total of 8.4 percent of our cohort had CDI scores greater than or equal to13, indicative of clinically significant depressive symptoms, with a range of 0-34. Depression scores were not associated with gender, age, or HbA1C. However, association with duration of diabetes showed a trend toward statistical significance (adjusted p = 0.068). Screening for depression using the CDI with a cutoff of 13 had similar positive testing rates as screening for microalbuminuria, hypercholesterolemia, thyroid dysfunction, and celiac sprue; in contrast, none of the clinic patients had evidence of retinopathy at their last ophthalmologic examinations. These findings indicate that screening for depression in a pediatric diabetes clinic identifies a substantial number of youngsters with high levels of depressive symptoms and has a yield that is equal to or greater than other standard screening tests and examinations. Thus, screening appears to be warranted.</p></body></sub-article><sub-article id="d31e2923"><front-stub><title-group><article-title>The Effect of Aging on the Innate Immune Response of Vascular Smooth Muscle</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Winterbottom</surname><given-names>Christopher</given-names></name></contrib><contrib contrib-type="author"><name><surname>Tesar</surname><given-names>Bethany</given-names></name></contrib><contrib contrib-type="author"><name><surname>Pearson</surname><given-names>Ellegant</given-names></name></contrib><contrib contrib-type="author"><name><surname>Rodov</surname><given-names>Sofya</given-names></name></contrib><contrib contrib-type="author"><name><surname>Goldstein</surname><given-names>Daniel</given-names></name></contrib></contrib-group><aff>Section of Cardiology, Department of Internal Medicine, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>As our understanding of atherosclerosis has become more sophisticated, a picture of the disease has emerged that emphasizes the role of inflammation in the pathogenesis of the disease, including signaling cascades mediated by toll-like receptors (TLRs). Data also has emerged suggesting that the process of aging plays an important role in atherogenesis, through pathways at least partially mediated by changes in the function of vascular smooth muscle cells. The purpose of this study was to elucidate the role that aging might play in the TLR-dependent innate immune response of vascular smooth muscle. Ex vivo cell and tissue culture models were used, with cytokine and chemokine production in response to stimulation with TLR agonists measured by ELISA assays. Our study demonstrated differences between aged and young cells and tissues in the production of the pro-inflammatory cytokines interleukin 6 (IL-6) and monocyte chemoattractant protein 1 (MCP-1). Greater production of IL-6 and less production of MCP-1 was seen in the aged specimens than in the young specimens. We discuss these results in the context of aging-related alterations in the innate immunity of vascular smooth muscle.</p></body></sub-article><sub-article id="d31e2964"><front-stub><title-group><article-title>Shock Index as a Predictor of Vasopressor Use in Severe Sepsis Patients in the Emergency Department</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Wollan</surname><given-names>Melissa</given-names></name></contrib></contrib-group><aff>Section of Emergency Medicine, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>Sepsis is considered to be a leading cause of non-coronary death in hospitals across the United States. Early identification and risk stratification is difficult because there is limited research characterizing patients at risk for disease progression who will require an escalation of care. Furthermore, patients often present with stable vital signs, masking the severity of illness at presentation. The aim of our study is to evaluate whether a Shock Index (SI) of greater than 0.8 (heart rate/systolic blood pressure) is a predictor of short-term vasopressor dependence in emergency department (ED) patients with severe sepsis.</p><p>A retrospective study was conducted using patients from the Yale-New Haven Hospital sepsis registry that presented to the YNHH Emergency Department between July 1, 2005, and July 30, 2007. ED patients were included if they had all of the following criteria: (1) A minimum two of four Systemic Inflammatory Response Syndrome (SIRS) criteria, (2) a documented or presumed source of infection, and (3) signs of end-organ dysfunction at presentation. Exclusion criteria were an age of younger than 18 years, presenting in extremis, being discharged home from the ED, not meeting severe sepsis criteria, or receiving comfort care at initial presentation. Patients were stratified into two groups: (1) Sustained SI elevation defined as a SI of greater than 0.8 for 80 percent or more of vital sign measurements and (2) Non-sustained SI, with no SI elevation or a SI elevation less than 80 percent of vital sign measurements. The principle outcome measure was the administration of vasopressors within 72 hours of initial presentation. Secondary outcomes included hospital mortality rate, organ failure scores, interventional therapies, and ED disposition.</p><p>Two hundred ninety-five of 359 patients met inclusion criteria for the study. Eighteen percent (n = 64) were excluded. The mean age was 62.5 years (SD &#x000b1; 18.54), with a difference between the sustained and non-sustained SI groups (56.5 vs. 67.9 years, respectively) (p &#x0003c; 0.05). Sixty-four percent (n = 189) presented with an initial elevated SI, while 47.5 percent (n = 140) maintained an elevated SI more than 80 percent of the time. A total of 24.4 percent (72 of 295) received vasopressors within 72 hours of presentation. Among patients with a sustained SI elevation, 38.6 percent (54 of 140) required vasopressors within 72 hours, contrasted to 11.6 percent (18 of 155) in the group without a sustained elevated SI (p &#x0003c; 0.05). The total percentage of time patients maintained an elevated SI greater than 0.8 was also associated with vasopressor use. The mean number of organ failures was 4.0 &#x000b1; 2.1 vs. 3.2 &#x000b1; 1.6, respectively, (p &#x0003c; 0.05) and was associated with mortality. Overall mortality for patients with a sustained SI was 19.3 percent (27 of 140), compared to 12.3 percent (19 of 155) in the non-sustained SI group (p &#x0003e; 0.05). Patients with an elevated SI were more likely to be admitted to the Intensive Care Unit or Step down Unit (53 vs. 41 percent, respectively; p &#x0003c; 0.05).</p><p>ED patients with severe sepsis and a sustained SI elevation have a significantly higher rate of vasopressor use within 72 hours of initial presentation than patients without a sustained SI elevation. Patients with an elevated SI also have a greater number of organ failures and a higher percentage of ICU admissions, both of which have been shown to correlate with higher in-hospital mortality rates. An elevated SI may be a useful modality for identifying patients with severe sepsis at risk for requiring escalation of care.</p></body></sub-article><sub-article id="d31e2987"><front-stub><title-group><article-title>Role of Biomarkers in Monitoring Gaucher Disease and the Potential of Biomarkers to Illuminate Pathophysiologic Pathways</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Yu</surname><given-names>Hannah J.</given-names></name><xref ref-type="aff" rid="E1">a</xref></contrib><contrib contrib-type="author"><name><surname>Cole</surname><given-names>J. Alexander</given-names></name><xref ref-type="aff" rid="E2">b</xref></contrib><contrib contrib-type="author"><name><surname>Yang</surname><given-names>Ruhua</given-names></name><xref ref-type="aff" rid="E1">a</xref></contrib><contrib contrib-type="author"><name><surname>Mistry</surname><given-names>Pramod K.</given-names></name><xref ref-type="aff" rid="E1">a</xref></contrib></contrib-group><aff id="E1"><label>a</label>Departments of Pediatrics and Internal Medicine, Yale University School of Medicine, New Haven, Connecticut</aff><aff id="E2"><label>b</label>Genzyme Corporation, Cambridge, Massachusetts</aff></front-stub><body><p>An observational study was conducted to assess the utility of biomarkers as a surrogate endpoint and a reflection of total body burden of Gaucher cells by examining correlations between biomarkers and liver volume and characterizing biomarker response to enzyme replacement therapy (ERT), as well as to gain insight into pathophysiologic pathways of Gaucher disease (GD).</p><p>One hundred fourteen patients with both pre- and post-treatment data were identified. These patients were further subdivided into intact spleen and asplenic patients. Differences in means among the subgroups of patients were determined, then regression analyses were run to investigate correlations between biomarkers and liver volume. Multiple serial measurements of liver volume, spleen volume, and chitotriosidase, while undergoing ERT, were graphed in a subgroup of five patients.</p><p>ACE, ferritin, transferrin saturation percentage, platelet count, and white blood cell count are all significantly increased in asplenic patients compared to intact spleen patients. Correlations with liver volume and biomarkers were weak, but some were significant: In intact spleen patients, liver volume was positively correlated with chitotriosidase and ACE, but negatively correlated with HDL, LDL, hemoglobin, and white blood cell count. In asplenic patients, liver volume was positively correlated with ACE and platelets. Chitotriosidase vs. liver volume and spleen volume responses to ERT showed a sigmoid curve.</p><p>This study shows that certain biomarker levels are increased in asplenic GD patients, suggesting that the spleen normally removes these substances during circulation. In addition, there were weak and inconsistent correlations between biomarkers and liver volume. These results, in addition the relationship between spleen and liver volume and chitotriosidase levels, suggest that chitotriosidase and other biomarkers are also excreted from other organs.</p></body></sub-article><sub-article id="d31e3042"><front-stub><title-group><article-title>BDNF Variants Influence Educational Attainment But Not Disease Characteristics in Alzheimer&#x02019;s Disease</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zdanys</surname><given-names>Kristina F.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Kleiman</surname><given-names>Timothy G.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Huiping</given-names></name></contrib><contrib contrib-type="author"><name><surname>Ozbay</surname><given-names>Fatih</given-names></name></contrib><contrib contrib-type="author"><name><surname>MacAvoy</surname><given-names>Martha G.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Gelernter</surname><given-names>Joel</given-names></name></contrib><contrib contrib-type="author"><name><surname>van Dyck</surname><given-names>Christopher H.</given-names></name></contrib></contrib-group><aff>Department of Psychiatry, Yale University School of Medicine, New Haven, Connecticut</aff></front-stub><body><p>This study aimed to determine whether brain-derived neurotrophic factor (BDNF) variants are related to premorbid educational attainment, progression of cognitive and functional decline, and associated neuropsychiatric symptoms in patients with Alzheimer&#x02019;s disease (AD). Three hundred forty-one AD subjects were genotyped for the BDNF polymorphisms val66met, C270T, and G-712A. Subjects received tests of cognition and daily function at baseline and at multiple subsequent time points during their participation. The Mini-Mental State Examination (MMSE) and Alzheimer&#x02019;s Disease Assessment Scale (ADAS-Cog) were used to measure cognition. Functional performance was assessed using the Instrumental Activities of Daily Living questionnaire (IADL), as well as the Alzheimer&#x02019;s Disease Cooperative Study-Activities of Daily Living inventory (ADCS-ADL). Subjects also were characterized for the frequency and severity of neuropsychiatric symptoms using the Neuropsychiatric Inventory (NPI). There was a significant effect of val66met genotype on educational attainment (F = 7.42, df = 2, 329, p = 0.00070), with met homozygotes having significantly fewer years of education than both the val/met and val/val groups. No association was observed between any BDNF polymorphism and measures of cognitive or functional decline. The C270T-T allele was associated with a higher prevalence of neuropsychiatric symptoms (Z = -2.11, N = 241, p = .035), specifically with the presence of hallucinations (OR = 3.25, 95 percent CI = 1.22-8.62, p = .018). In summary, the val66met polymorphism appears to be associated with lower premorbid educational attainment in AD patients. The C270T-T allele demonstrated association with total neuropsychiatric symptoms, specifically hallucinations. BDNF genotypes in this sample do not confer a more rapid rate of cognitive or functional decline.</p></body></sub-article></article>